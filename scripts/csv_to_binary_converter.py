#!/usr/bin/env python3
"""
é«˜æ•ˆç¦»çº¿é¢„å¤„ç†è„šæœ¬
å°†å¤§å‹CSVæ–‡ä»¶ä¸€æ¬¡æ€§è½¬æ¢ä¸ºé«˜æ•ˆçš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œæ”¯æŒçœŸæ­£çš„éšæœºè®¿é—®

ä½¿ç”¨æ–¹æ³•:
    python csv_to_binary_converter.py --input /path/to/feature_daily_ts.csv --output /path/to/binary_data
"""

import numpy as np
import pandas as pd
import pickle
import logging
from pathlib import Path
from tqdm import tqdm
import json
import argparse
import sys
import os
# å¯¼å…¥æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from ..data_quality_checker import check_qout_data, check_input_features, check_target_data

class CSVToBinaryConverter:
    """CSVåˆ°äºŒè¿›åˆ¶æ ¼å¼çš„é«˜æ•ˆè½¬æ¢å™¨ - å¸¦å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥"""
    
    def __init__(self, csv_path: str, output_dir: str, chunk_size: int = 100000, 
                 enable_data_check: bool = True, fix_anomalies: bool = False,
                 input_features: list = None, attr_features: list = None,
                 target_cols: list = None):
        self.csv_path = csv_path
        self.output_dir = Path(output_dir)
        self.chunk_size = chunk_size
        self.enable_data_check = enable_data_check
        self.fix_anomalies = fix_anomalies
        self.input_features = input_features or []
        self.attr_features = attr_features or []
        self.target_cols = target_cols or ['TN', 'TP']
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®°å½•æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ
        self.quality_report = {
            'total_anomalies': 0,
            'fixed_anomalies': 0,
            'check_results': {}
        }
        
        logging.info(f"åˆå§‹åŒ–è½¬æ¢å™¨: {csv_path} -> {output_dir}")
        if enable_data_check:
            logging.info("å·²å¯ç”¨å…¨é‡æ•°æ®è´¨é‡æ£€æŸ¥")
            logging.info(f"æ•°æ®ä¿®å¤æ¨¡å¼: {'å¼€å¯' if fix_anomalies else 'å…³é—­'}")
        
    def convert(self):
        """æ‰§è¡Œè½¬æ¢çš„ä¸»å‡½æ•°"""
        logging.info(f"å¼€å§‹è½¬æ¢å¤§å‹CSVæ–‡ä»¶...")
        
        # ç¬¬ä¸€éï¼šåˆ†ææ•°æ®ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
        metadata = self._analyze_structure()
        
        # ç¬¬äºŒéï¼šè½¬æ¢æ•°æ®ä¸ºNumPyæ ¼å¼ï¼ˆåŒ…å«æ•°æ®è´¨é‡æ£€æŸ¥ï¼‰
        data_arrays, comid_index = self._convert_to_binary_with_quality_check(metadata)
        
        # ä¿å­˜ç»“æœ
        self._save_binary_data(data_arrays, comid_index, metadata)
        
        # ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        self._save_quality_report()
        
        logging.info("è½¬æ¢å®Œæˆï¼")
        return self.output_dir
    
    def _analyze_structure(self):
        """åˆ†æCSVç»“æ„ï¼Œç¡®å®šæ•°æ®ç±»å‹å’Œç»´åº¦"""
        logging.info("ç¬¬ä¸€éæ‰«æï¼šåˆ†ææ•°æ®ç»“æ„...")
        
        # è¯»å–ç¬¬ä¸€ä¸ªå°å—æ¥åˆ†æç»“æ„
        first_chunk = next(pd.read_csv(self.csv_path, chunksize=1000))
        
        # ç¡®å®šåˆ—ç±»å‹
        numeric_cols = []
        categorical_cols = []
        
        for col in first_chunk.columns:
            if col in ['COMID', 'date']:
                continue  # ç‰¹æ®Šå¤„ç†
            elif first_chunk[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                numeric_cols.append(col)
            else:
                categorical_cols.append(col)
        
        # ç»Ÿè®¡æ€»è¡Œæ•°å’ŒCOMIDä¿¡æ¯
        total_rows = 0
        unique_comids = set()
        date_range = {'min': None, 'max': None}
        
        chunk_iter = pd.read_csv(self.csv_path, chunksize=self.chunk_size)
        for chunk in tqdm(chunk_iter, desc="æ‰«ææ•°æ®ç»“æ„"):
            total_rows += len(chunk)
            unique_comids.update(chunk['COMID'].astype(str))
            
            if 'date' in chunk.columns:
                chunk_dates = pd.to_datetime(chunk['date'])
                if date_range['min'] is None:
                    date_range['min'] = chunk_dates.min()
                    date_range['max'] = chunk_dates.max()
                else:
                    date_range['min'] = min(date_range['min'], chunk_dates.min())
                    date_range['max'] = max(date_range['max'], chunk_dates.max())
        
        metadata = {
            'total_rows': total_rows,
            'numeric_cols': numeric_cols,
            'categorical_cols': categorical_cols,
            'unique_comids': sorted(list(unique_comids)),
            'date_range': {
                'min': date_range['min'].isoformat() if date_range['min'] else None,
                'max': date_range['max'].isoformat() if date_range['max'] else None
            },
            'n_comids': len(unique_comids),
            'n_numeric_features': len(numeric_cols)
        }
        
        logging.info(f"æ•°æ®åˆ†æå®Œæˆ:")
        logging.info(f"  - æ€»è¡Œæ•°: {total_rows:,}")
        logging.info(f"  - COMIDæ•°é‡: {len(unique_comids):,}")
        logging.info(f"  - æ•°å€¼ç‰¹å¾: {len(numeric_cols)} åˆ—")
        logging.info(f"  - æ—¶é—´èŒƒå›´: {date_range['min']} åˆ° {date_range['max']}")
        
        return metadata
    
    def _convert_to_binary_with_quality_check(self, metadata):
        """å°†CSVè½¬æ¢ä¸ºé«˜æ•ˆçš„äºŒè¿›åˆ¶æ ¼å¼ï¼ˆå¸¦å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥ï¼‰"""
        logging.info("ç¬¬äºŒéæ‰«æï¼šè½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼ + å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        total_rows = metadata['total_rows']
        numeric_cols = metadata['numeric_cols']
        
        # é¢„åˆ†é…NumPyæ•°ç»„
        numeric_data = np.empty((total_rows, len(numeric_cols)), dtype=np.float32)
        comid_array = np.empty(total_rows, dtype='<U20')  # å­—ç¬¦ä¸²æ•°ç»„
        date_array = np.empty(total_rows, dtype='datetime64[D]')
        
        # COMIDç´¢å¼•ï¼šCOMID -> [(start_idx, end_idx), ...]
        comid_index = {}
        
        current_row = 0
        chunk_iter = pd.read_csv(self.csv_path, chunksize=self.chunk_size)
        
        for chunk in tqdm(chunk_iter, desc="è½¬æ¢æ•°æ® + æ•°æ®è´¨é‡æ£€æŸ¥", total=total_rows//self.chunk_size + 1):
            chunk_size = len(chunk)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if self.enable_data_check:
                chunk = self._perform_quality_check(chunk)
            
            # å¤„ç†æ•°å€¼æ•°æ®
            numeric_data[current_row:current_row + chunk_size] = chunk[numeric_cols].values.astype(np.float32)
            
            # å¤„ç†COMID
            comids = chunk['COMID'].astype(str).values
            comid_array[current_row:current_row + chunk_size] = comids
            
            # å¤„ç†æ—¥æœŸ
            if 'date' in chunk.columns:
                dates = pd.to_datetime(chunk['date']).values.astype('datetime64[D]')
                date_array[current_row:current_row + chunk_size] = dates
            
            # æ›´æ–°COMIDç´¢å¼•
            for comid in np.unique(comids):
                comid_positions = np.where(comids == comid)[0] + current_row
                if comid not in comid_index:
                    comid_index[comid] = []
                comid_index[comid].extend(comid_positions.tolist())
            
            current_row += chunk_size
        
        # ä¼˜åŒ–COMIDç´¢å¼•ï¼šè½¬æ¢ä¸ºè¿ç»­åŒºé—´
        logging.info("ä¼˜åŒ–COMIDç´¢å¼•...")
        optimized_index = {}
        for comid, positions in tqdm(comid_index.items(), desc="ä¼˜åŒ–ç´¢å¼•"):
            positions = sorted(positions)
            ranges = []
            if len(positions) == 0:
                continue
                
            start = positions[0]
            end = positions[0]
            
            for pos in positions[1:]:
                if pos == end + 1:
                    end = pos
                else:
                    ranges.append((start, end + 1))  # end+1 for Python slicing
                    start = end = pos
            ranges.append((start, end + 1))
            optimized_index[comid] = ranges
        
        return {
            'numeric_data': numeric_data,
            'comid_array': comid_array,
            'date_array': date_array,
            'columns': numeric_cols
        }, optimized_index
    
    def _perform_quality_check(self, chunk):
        """å¯¹æ•°æ®å—æ‰§è¡Œå…¨é¢çš„è´¨é‡æ£€æŸ¥"""
        original_chunk = chunk.copy()
        
        try:
            # 1. æ£€æŸ¥æµé‡æ•°æ® (Qout)
            if 'Qout' in chunk.columns:
                chunk, qout_results = check_qout_data(
                    chunk,
                    fix_anomalies=self.fix_anomalies,
                    verbose=False,  # æ‰¹é‡å¤„ç†æ—¶å‡å°‘æ—¥å¿—
                    logger=logging,
                    data_type='timeseries'
                )
                self._accumulate_check_results('Qout', qout_results)
            
            # 2. æ£€æŸ¥è¾“å…¥ç‰¹å¾
            available_input_features = [col for col in self.input_features if col in chunk.columns]
            if available_input_features:
                chunk, input_results = check_input_features(
                    chunk,
                    input_features=available_input_features,
                    fix_anomalies=self.fix_anomalies,
                    verbose=False,
                    logger=logging,
                    data_type='timeseries'
                )
                self._accumulate_check_results('input_features', input_results)
            
            # 3. æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ®
            available_target_cols = [col for col in self.target_cols if col in chunk.columns]
            if available_target_cols:
                chunk, target_results = check_target_data(
                    chunk,
                    target_cols=available_target_cols,
                    fix_anomalies=False,  # æ°´è´¨æ•°æ®ä¸è‡ªåŠ¨å¡«å……
                    verbose=False,
                    logger=logging,
                    data_type='timeseries'
                )
                self._accumulate_check_results('target_data', target_results)
            
        except Exception as e:
            logging.warning(f"æ•°æ®å—è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
            return original_chunk
        
        return chunk
    
    def _accumulate_check_results(self, check_type, results):
        """ç´¯ç§¯è´¨é‡æ£€æŸ¥ç»“æœ"""
        if check_type not in self.quality_report['check_results']:
            self.quality_report['check_results'][check_type] = {
                'total_checks': 0,
                'anomalies_found': 0,
                'anomalies_fixed': 0
            }
        
        result_summary = self.quality_report['check_results'][check_type]
        result_summary['total_checks'] += 1
        
        if results.get('has_anomalies', False):
            result_summary['anomalies_found'] += 1
            self.quality_report['total_anomalies'] += 1
            
            if self.fix_anomalies:
                result_summary['anomalies_fixed'] += 1
                self.quality_report['fixed_anomalies'] += 1
    
    def _save_binary_data(self, data_arrays, comid_index, metadata):
        """ä¿å­˜äºŒè¿›åˆ¶æ•°æ®å’Œç´¢å¼•"""
        logging.info("ä¿å­˜äºŒè¿›åˆ¶æ–‡ä»¶...")
        
        # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶ï¼ˆå…³é”®ï¼šä½¿ç”¨å†…å­˜æ˜ å°„ï¼‰
        np.save(self.output_dir / 'numeric_data.npy', data_arrays['numeric_data'])
        np.save(self.output_dir / 'comid_array.npy', data_arrays['comid_array'])
        np.save(self.output_dir / 'date_array.npy', data_arrays['date_array'])
        
        # ä¿å­˜COMIDç´¢å¼•
        with open(self.output_dir / 'comid_index.pkl', 'wb') as f:
            pickle.dump(comid_index, f)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_copy = metadata.copy()
        metadata_copy['columns'] = data_arrays['columns']  # æ·»åŠ åˆ—ä¿¡æ¯
        with open(self.output_dir / 'metadata.json', 'w') as f:
            json.dump(metadata_copy, f, indent=2, default=str)
        
        # ä¿å­˜åˆ—åï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œæ–¹ä¾¿åŠ è½½ï¼‰
        with open(self.output_dir / 'columns.json', 'w') as f:
            json.dump(data_arrays['columns'], f)
        
        # è®¡ç®—å’Œè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        data_size_mb = data_arrays['numeric_data'].nbytes / (1024**2)
        total_size_mb = sum([
            data_arrays['numeric_data'].nbytes,
            data_arrays['comid_array'].nbytes,
            data_arrays['date_array'].nbytes
        ]) / (1024**2)
        
        # ä¿å­˜è½¬æ¢ç»Ÿè®¡
        stats = {
            'conversion_completed': True,
            'numeric_data_size_mb': data_size_mb,
            'total_binary_size_mb': total_size_mb,
            'comid_count': len(comid_index),
            'compression_ratio': None,  # å¯ä»¥è®¡ç®—ä¸åŸCSVçš„å‹ç¼©æ¯”
            'file_format': 'numpy_binary'
        }
        
        with open(self.output_dir / 'conversion_stats.json', 'w') as f:
            json.dump(stats, f, indent=2)
        
        logging.info(f"âœ“ æ•°æ®è½¬æ¢å®Œæˆ!")
        logging.info(f"âœ“ æ•°å€¼æ•°æ®å¤§å°: {data_size_mb:.1f} MB")
        logging.info(f"âœ“ æ€»æ–‡ä»¶å¤§å°: {total_size_mb:.1f} MB")
        logging.info(f"âœ“ COMIDç´¢å¼•: {len(comid_index):,} ä¸ª")
        logging.info(f"âœ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _save_quality_report(self):
        """ä¿å­˜æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
        if not self.enable_data_check:
            return
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        self.quality_report['summary'] = {
            'data_check_enabled': self.enable_data_check,
            'fix_anomalies_enabled': self.fix_anomalies,
            'total_anomaly_rate': (self.quality_report['total_anomalies'] / 
                                 max(1, sum(r['total_checks'] for r in self.quality_report['check_results'].values()))),
            'fix_success_rate': (self.quality_report['fixed_anomalies'] / 
                               max(1, self.quality_report['total_anomalies'])) if self.quality_report['total_anomalies'] > 0 else 0
        }
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        quality_file = self.output_dir / 'data_quality_report.json'
        with open(quality_file, 'w', encoding='utf-8') as f:
            json.dump(self.quality_report, f, indent=2, ensure_ascii=False)
        
        # è¾“å‡ºè´¨é‡æ£€æŸ¥æ‘˜è¦
        logging.info("=" * 60)
        logging.info("ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥æ‘˜è¦:")
        logging.info("=" * 60)
        logging.info(f"âœ“ æ•°æ®è´¨é‡æ£€æŸ¥: {'å·²å¯ç”¨' if self.enable_data_check else 'å·²ç¦ç”¨'}")
        logging.info(f"âœ“ å¼‚å¸¸æ•°æ®ä¿®å¤: {'å·²å¯ç”¨' if self.fix_anomalies else 'å·²ç¦ç”¨'}")
        logging.info(f"âœ“ æ£€æŸ¥çš„æ•°æ®å—: {sum(r['total_checks'] for r in self.quality_report['check_results'].values())} ä¸ª")
        logging.info(f"âœ“ å‘ç°å¼‚å¸¸æ•°æ®å—: {self.quality_report['total_anomalies']} ä¸ª")
        if self.fix_anomalies and self.quality_report['total_anomalies'] > 0:
            logging.info(f"âœ“ ä¿®å¤å¼‚å¸¸æ•°æ®å—: {self.quality_report['fixed_anomalies']} ä¸ª")
        
        for check_type, results in self.quality_report['check_results'].items():
            anomaly_rate = results['anomalies_found'] / max(1, results['total_checks'])
            logging.info(f"  - {check_type}: {results['anomalies_found']}/{results['total_checks']} å¼‚å¸¸ ({anomaly_rate:.1%})")
        
        logging.info(f"âœ“ è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {quality_file}")
        logging.info("=" * 60)


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('csv_conversion.log')
        ]
    )


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CSVåˆ°äºŒè¿›åˆ¶æ ¼å¼è½¬æ¢å™¨ - ä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ– (å¸¦å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥)')
    parser.add_argument('--input', required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--chunk-size', type=int, default=100000, help='å¤„ç†å—å¤§å° (é»˜è®¤: 100000)')
    
    # æ•°æ®è´¨é‡æ£€æŸ¥é€‰é¡¹
    parser.add_argument('--enable-data-check', action='store_true', default=True, 
                       help='å¯ç”¨å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥ (é»˜è®¤: True)')
    parser.add_argument('--disable-data-check', action='store_true',
                       help='ç¦ç”¨æ•°æ®è´¨é‡æ£€æŸ¥')
    parser.add_argument('--fix-anomalies', action='store_true', 
                       help='è‡ªåŠ¨ä¿®å¤æ£€æµ‹åˆ°çš„å¼‚å¸¸æ•°æ® (é»˜è®¤: False)')
    
    # ç‰¹å¾é…ç½®é€‰é¡¹
    parser.add_argument('--input-features', nargs='*', 
                       default=['TN', 'TP', 'Qout', 'precipitation', 'temperature_2m_mean', 'runoff'],
                       help='è¾“å…¥ç‰¹å¾åˆ—è¡¨')
    parser.add_argument('--target-cols', nargs='*', 
                       default=['TN', 'TP'],
                       help='ç›®æ ‡åˆ—åˆ—è¡¨')
    
    args = parser.parse_args()
    
    # å¤„ç†æ•°æ®æ£€æŸ¥é€‰é¡¹
    enable_data_check = args.enable_data_check and not args.disable_data_check
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    file_size_gb = os.path.getsize(args.input) / (1024**3)
    logging.info(f"è¾“å…¥æ–‡ä»¶å¤§å°: {file_size_gb:.1f} GB")
    
    if file_size_gb > 50:
        logging.warning(f"è¾“å…¥æ–‡ä»¶å¾ˆå¤§ ({file_size_gb:.1f} GB)ï¼Œè½¬æ¢å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
    
    # åˆ›å»ºè½¬æ¢å™¨ï¼ˆå¸¦æ•°æ®è´¨é‡æ£€æŸ¥åŠŸèƒ½ï¼‰
    converter = CSVToBinaryConverter(
        csv_path=args.input,
        output_dir=args.output,
        chunk_size=args.chunk_size,
        enable_data_check=enable_data_check,
        fix_anomalies=args.fix_anomalies,
        input_features=args.input_features,
        target_cols=args.target_cols
    )
    
    try:
        # æ‰§è¡Œè½¬æ¢
        output_dir = converter.convert()
        
        logging.info("=" * 60)
        logging.info("âœ“ è½¬æ¢æˆåŠŸå®Œæˆï¼")
        logging.info("=" * 60)
        logging.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logging.info("")
        logging.info("æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨é«˜æ•ˆæ•°æ®åŠ è½½å™¨:")
        logging.info("from efficient_data_loader import EfficientDataLoader")
        logging.info(f"loader = EfficientDataLoader('{output_dir}')")
        
    except Exception as e:
        logging.error(f"è½¬æ¢å¤±è´¥: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        sys.exit(1)


if __name__ == '__main__':
    main()