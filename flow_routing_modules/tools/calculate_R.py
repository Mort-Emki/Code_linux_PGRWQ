"""
ä¼˜åŒ–çš„ä¿ç•™ç³»æ•°è®¡ç®—ç¨‹åº - åŸºäºæ‹†åˆ†æ•°æ®çš„é«˜æ€§èƒ½ç‰ˆæœ¬

è¿™ä¸ªç¨‹åºä½¿ç”¨01_split_daily_data_by_comid.pyäº§ç”Ÿçš„æ‹†åˆ†æ•°æ®ï¼Œ
å®ç°O(1)å¤æ‚åº¦çš„æ•°æ®è®¿é—®ï¼Œå¤§å¹…æå‡è®¡ç®—æ€§èƒ½ã€‚

æ€§èƒ½æå‡ï¼š
- æ—¶é—´å¤æ‚åº¦ï¼šä»O(MÃ—N)é™ä½åˆ°O(M)
- é¢„æœŸè¿è¡Œæ—¶é—´ï¼šå‡å°‘80-95%
- å†…å­˜ä½¿ç”¨ï¼šå‡å°‘70-90%

ä½œè€…: [Your Name]
æ—¥æœŸ: 2025-01-XX
"""

import pandas as pd
import numpy as np
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
from tqdm import tqdm

# å¯¼å…¥åŸæœ‰çš„è®¡ç®—å‡½æ•°
from ..core.geometry import get_river_length, calculate_river_width
from ..physics.environment_param import compute_retainment_factor
from ...data_processing import detect_and_handle_anomalies

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('retention_calculation_optimized.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class OptimizedDataLoader:
    """
    ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨ - æ”¯æŒå¿«é€Ÿè®¿é—®æ‹†åˆ†åçš„COMIDæ•°æ®
    """
    
    def __init__(self, split_data_dir: str):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        å‚æ•°:
            split_data_dir: æ‹†åˆ†æ•°æ®çš„ç›®å½•è·¯å¾„
        """
        self.split_data_dir = Path(split_data_dir)
        self.comid_files_dir = self.split_data_dir / "comid_files"
        
        # åŠ è½½ç´¢å¼•æ–‡ä»¶
        self.index_file = self.split_data_dir / "comid_index.json"
        self.metadata_file = self.split_data_dir / "split_metadata.json"
        
        if not self.index_file.exists():
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_file}")
        
        with open(self.index_file, 'r', encoding='utf-8') as f:
            self.comid_index = json.load(f)
        
        # åŠ è½½å…ƒæ•°æ®
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {}
        
        # æ•°æ®ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        self._cache = {}
        self._cache_size_limit = 100  # æœ€å¤šç¼“å­˜100ä¸ªCOMIDçš„æ•°æ®
        
        logging.info(f"æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logging.info(f"å¯ç”¨COMIDæ•°é‡: {len(self.comid_index)}")
        logging.info(f"æ•°æ®æ–‡ä»¶ç›®å½•: {self.comid_files_dir}")
    
    def load_comid_data(self, comid: int, use_cache: bool = True) -> pd.DataFrame:
        """
        å¿«é€ŸåŠ è½½æŒ‡å®šCOMIDçš„æ•°æ® - O(1)å¤æ‚åº¦ï¼
        
        å‚æ•°:
            comid: COMIDç¼–å·
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        è¿”å›:
            DataFrame: è¯¥COMIDçš„æ‰€æœ‰æ—¶é—´åºåˆ—æ•°æ®
        """
        comid_str = str(comid)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and comid_str in self._cache:
            return self._cache[comid_str].copy()
        
        # æ£€æŸ¥COMIDæ˜¯å¦å­˜åœ¨
        if comid_str not in self.comid_index:
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        filename = self.comid_index[comid_str]
        filepath = self.comid_files_dir / filename
        
        if not filepath.exists():
            logging.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return pd.DataFrame()
        
        try:
            # æ ¹æ®æ–‡ä»¶æ ¼å¼è¯»å–æ•°æ®
            if filename.endswith('.parquet'):
                data = pd.read_parquet(filepath)
            elif filename.endswith('.csv') or filename.endswith('.csv.gz'):
                data = pd.read_csv(filepath)
            elif filename.endswith('.feather'):
                data = pd.read_feather(filepath)
            else:
                logging.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
                return pd.DataFrame()
            
            # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeæ ¼å¼
            if 'date' in data.columns:
                data['date'] = pd.to_datetime(data['date'])
            
            # ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
            if use_cache:
                # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
                if len(self._cache) >= self._cache_size_limit:
                    oldest_key = next(iter(self._cache))
                    del self._cache[oldest_key]
                
                self._cache[comid_str] = data.copy()
            
            return data
            
        except Exception as e:
            logging.error(f"è¯»å–COMID {comid} æ•°æ®æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    
    def get_comid_info(self, comid: int) -> Dict:
        """
        è·å–COMIDçš„å…ƒä¿¡æ¯
        
        å‚æ•°:
            comid: COMIDç¼–å·
        
        è¿”å›:
            dict: åŒ…å«è®°å½•æ•°ã€æ—¥æœŸèŒƒå›´ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        comid_str = str(comid)
        
        if 'file_info' in self.metadata and comid_str in self.metadata['file_info']:
            return self.metadata['file_info'][comid_str]
        else:
            return {}
    
    def get_available_comids(self) -> List[int]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„COMIDåˆ—è¡¨
        
        è¿”å›:
            List[int]: COMIDåˆ—è¡¨
        """
        return [int(comid) for comid in self.comid_index.keys()]
    
    def get_common_dates(self, comid1: int, comid2: int) -> List[pd.Timestamp]:
        """
        è·å–ä¸¤ä¸ªCOMIDçš„å…±åŒæ—¥æœŸ
        
        å‚æ•°:
            comid1, comid2: ä¸¤ä¸ªCOMIDç¼–å·
        
        è¿”å›:
            List[pd.Timestamp]: å…±åŒæ—¥æœŸåˆ—è¡¨
        """
        data1 = self.load_comid_data(comid1)
        data2 = self.load_comid_data(comid2)
        
        if data1.empty or data2.empty:
            return []
        
        if 'date' not in data1.columns or 'date' not in data2.columns:
            return []
        
        # æ‰¾åˆ°å…±åŒçš„æ—¥æœŸ
        dates1 = set(data1['date'])
        dates2 = set(data2['date'])
        common_dates = sorted(dates1 & dates2)
        
        return common_dates


def load_river_attributes_optimized(attr_data_path: str) -> Tuple[Dict, Dict]:
    """
    åŠ è½½æ²³æ®µå±æ€§æ•°æ®å¹¶æ„å»ºä¼˜åŒ–çš„æŸ¥æ‰¾å­—å…¸
    
    å‚æ•°:
        attr_data_path: æ²³æ®µå±æ€§æ•°æ®æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        Tuple[Dict, Dict]: (æ‹“æ‰‘å­—å…¸, å±æ€§å­—å…¸)
    """
    
    logging.info("åŠ è½½æ²³æ®µå±æ€§æ•°æ®...")
    
    try:
        attr_df = pd.read_csv(attr_data_path)
        logging.info(f"æ²³æ®µå±æ€§æ•°æ®å½¢çŠ¶: {attr_df.shape}")
        
        # æ„å»ºæ‹“æ‰‘å­—å…¸ (COMID -> NextDownID)
        topo_dict = attr_df.set_index('COMID')['NextDownID'].to_dict()
        
        # æ„å»ºå±æ€§å­—å…¸
        attr_dict = {}
        for _, row in attr_df.iterrows():
            comid_int = int(row['COMID'])
            attr_dict[str(comid_int)] = {
                'lengthkm': row.get('lengthkm', 1.0),
                'order_': row.get('order_', 1),
                'slope': row.get('slope', 0.001),
                'uparea': row.get('uparea', 1.0)
            }
        
        logging.info(f"æˆåŠŸåŠ è½½ {len(attr_dict)} ä¸ªæ²³æ®µçš„å±æ€§æ•°æ®")
        
        return topo_dict, attr_dict
        
    except Exception as e:
        logging.error(f"åŠ è½½æ²³æ®µå±æ€§æ•°æ®æ—¶å‡ºé”™: {e}")
        raise


def perform_data_quality_check(data_loader: OptimizedDataLoader, sample_size: int = 100):
    """
    å¯¹æ‹†åˆ†åçš„æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥
    
    å‚æ•°:
        data_loader: æ•°æ®åŠ è½½å™¨
        sample_size: æŠ½æ ·æ£€æŸ¥çš„COMIDæ•°é‡
    """
    
    logging.info("=" * 60)
    logging.info("å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥")
    logging.info("=" * 60)
    
    available_comids = data_loader.get_available_comids()
    
    if len(available_comids) == 0:
        logging.error("æ²¡æœ‰å¯ç”¨çš„COMIDæ•°æ®")
        return
    
    # éšæœºæŠ½æ ·æ£€æŸ¥
    import random
    sample_comids = random.sample(available_comids, min(sample_size, len(available_comids)))
    
    total_records = 0
    total_missing = 0
    date_ranges = []
    
    for comid in tqdm(sample_comids, desc="æ•°æ®è´¨é‡æ£€æŸ¥"):
        try:
            data = data_loader.load_comid_data(comid)
            
            if data.empty:
                logging.warning(f"COMID {comid} æ•°æ®ä¸ºç©º")
                continue
            
            # ç»Ÿè®¡è®°å½•æ•°
            total_records += len(data)
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_in_data = data.isnull().sum().sum()
            total_missing += missing_in_data
            
            # è®°å½•æ—¥æœŸèŒƒå›´
            if 'date' in data.columns:
                date_range = (data['date'].min(), data['date'].max())
                date_ranges.append(date_range)
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥COMID {comid} æ—¶å‡ºé”™: {e}")
    
    # æ±‡æ€»ç»Ÿè®¡
    if date_ranges:
        overall_start = min(dr[0] for dr in date_ranges)
        overall_end = max(dr[1] for dr in date_ranges)
        logging.info(f"æ•´ä½“æ—¥æœŸèŒƒå›´: {overall_start} åˆ° {overall_end}")
    
    logging.info(f"æŠ½æ ·æ£€æŸ¥å®Œæˆ:")
    logging.info(f"- æ£€æŸ¥COMIDæ•°: {len(sample_comids)}")
    logging.info(f"- æ€»è®°å½•æ•°: {total_records:,}")
    logging.info(f"- æ€»ç¼ºå¤±å€¼: {total_missing:,}")
    logging.info(f"- ç¼ºå¤±ç‡: {total_missing/max(total_records, 1)*100:.2f}%")


def calculate_retention_coefficients_optimized(
    split_data_dir: str,
    attr_data_path: str,
    output_dir: str = "output_optimized",
    parameters: List[str] = ["TN", "TP"],
    v_f_TN: float = 35.0,
    v_f_TP: float = 44.5,
    enable_anomaly_check: bool = True,
    fix_anomalies: bool = True,
    max_records_per_param: int = 10000000,
    progress_interval: int = 100
):
    """
    ä½¿ç”¨æ‹†åˆ†æ•°æ®çš„ä¼˜åŒ–ä¿ç•™ç³»æ•°è®¡ç®—
    
    å‚æ•°:
        split_data_dir: æ‹†åˆ†æ•°æ®ç›®å½•
        attr_data_path: æ²³æ®µå±æ€§æ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        parameters: è¦è®¡ç®—çš„å‚æ•°åˆ—è¡¨
        v_f_TN, v_f_TP: å¸æ”¶é€Ÿç‡å‚æ•°
        enable_anomaly_check: æ˜¯å¦å¯ç”¨å¼‚å¸¸å€¼æ£€æµ‹
        fix_anomalies: æ˜¯å¦ä¿®å¤å¼‚å¸¸å€¼
        max_records_per_param: æ¯ä¸ªå‚æ•°çš„æœ€å¤§è®°å½•æ•°
        progress_interval: è¿›åº¦æŠ¥å‘Šé—´éš”
    
    è¿”å›:
        dict: åŒ…å«æ¯ä¸ªå‚æ•°DataFrameçš„å­—å…¸
    """
    
    logging.info("=" * 80)
    logging.info("å¼€å§‹ä¼˜åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—")
    logging.info("=" * 80)
    
    start_time = datetime.now()
    
    # 1. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    try:
        data_loader = OptimizedDataLoader(split_data_dir)
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        raise
    
    # 2. æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
    if enable_anomaly_check:
        perform_data_quality_check(data_loader, sample_size=50)
    
    # 3. åŠ è½½æ²³æ®µå±æ€§æ•°æ®
    topo_dict, attr_dict = load_river_attributes_optimized(attr_data_path)
    
    # 4. è·å–å¯å¤„ç†çš„COMIDåˆ—è¡¨
    available_comids = data_loader.get_available_comids()
    processable_comids = [comid for comid in available_comids if comid in topo_dict]
    
    logging.info(f"å¯ç”¨COMIDæ€»æ•°: {len(available_comids)}")
    logging.info(f"å¯å¤„ç†COMIDæ•°: {len(processable_comids)}")
    
    # 5. åˆå§‹åŒ–ç»“æœå­˜å‚¨
    parameter_results = {param: [] for param in parameters}
    success_counts = {param: 0 for param in parameters}
    record_counts = {param: 0 for param in parameters}
    error_counts = 0
    
    terminal_segments = 0  # ç»ˆç«¯æ²³æ®µæ•°é‡
    empty_data_pairs = 0   # æ•°æ®ä¸ºç©ºçš„æ²³æ®µå¯¹æ•°é‡
    no_common_dates = 0    # æ²¡æœ‰å…±åŒæ—¥æœŸçš„æ²³æ®µå¯¹æ•°é‡
    processed_pairs = 0    # å®é™…å¤„ç†çš„æ²³æ®µå¯¹æ•°é‡

    # 6. ä¸»è®¡ç®—å¾ªç¯ - è¿™æ˜¯å…³é”®ä¼˜åŒ–éƒ¨åˆ†ï¼
    logging.info("å¼€å§‹ä¸»è®¡ç®—å¾ªç¯...")
    
    for i, comid in enumerate(tqdm(processable_comids, desc="è®¡ç®—ä¿ç•™ç³»æ•°")):
        

        # è·å–ä¸‹æ¸¸æ²³æ®µID
        next_down_id = topo_dict.get(comid, 0)
        next_down_id = int(next_down_id) if next_down_id else 0 
        if next_down_id == 0:
            terminal_segments += 1
            continue  # è·³è¿‡ç»ˆç«¯æ²³æ®µ
        
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šO(1)æ•°æ®è¯»å–ï¼Œè€ŒéO(N)æŸ¥æ‰¾ï¼
        up_data = data_loader.load_comid_data(comid)
        down_data = data_loader.load_comid_data(next_down_id)
        
        if up_data.empty or down_data.empty:
            empty_data_pairs += 1
            continue
        
        # ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿè·å–å…±åŒæ—¥æœŸ
        common_dates = data_loader.get_common_dates(comid, next_down_id)
        if not common_dates:
            no_common_dates += 1
            continue
        
        processed_pairs += 1
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è®°å½•é™åˆ¶
        if all(record_counts[param] >= max_records_per_param for param in parameters):
            logging.info(f"æ‰€æœ‰å‚æ•°éƒ½å·²è¾¾åˆ°æœ€å¤§è®°å½•æ•°é™åˆ¶ ({max_records_per_param})")
            break
        
        # è¿›åº¦æŠ¥å‘Š
        if (i + 1) % progress_interval == 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = (i + 1) / elapsed
            eta = (len(processable_comids) - i - 1) / rate if rate > 0 else 0
            logging.info(f"å¤„ç†è¿›åº¦: {i+1}/{len(processable_comids)} "
                        f"({(i+1)/len(processable_comids)*100:.1f}%), "
                        f"å¤„ç†é€Ÿåº¦: {rate:.1f} COMID/ç§’, "
                        f"é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
        
        try:
            # print(f"æ­£åœ¨å¤„ç†COMID: {comid} -> ä¸‹æ¸¸COMID: {next_down_id}")
            # è·å–ä¸‹æ¸¸æ²³æ®µID
            next_down_id = topo_dict.get(comid, 0)
            next_down_id = int(next_down_id) if next_down_id else 0
            if next_down_id == 0:
                print(f"è·³è¿‡ç»ˆç«¯æ²³æ®µ: {comid}")
                continue  # è·³è¿‡ç»ˆç«¯æ²³æ®µ
            
            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šO(1)æ•°æ®è¯»å–ï¼Œè€ŒéO(N)æŸ¥æ‰¾ï¼
            up_data = data_loader.load_comid_data(comid)
            down_data = data_loader.load_comid_data(next_down_id)
            
            if up_data.empty or down_data.empty:
                print(f"è·³è¿‡æ•°æ®ä¸ºç©ºçš„æ²³æ®µå¯¹: {comid} -> {next_down_id}")
                continue
            
            # ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿè·å–å…±åŒæ—¥æœŸ
            common_dates = data_loader.get_common_dates(comid, next_down_id)
            print(f"å…±åŒæ—¥æœŸæ•°é‡: {len(common_dates)}")
            if not common_dates:
                continue
            
            # ç­›é€‰å…±åŒæ—¥æœŸçš„æ•°æ®
            up_subset = up_data[up_data['date'].isin(common_dates)].set_index('date')
            down_subset = down_data[down_data['date'].isin(common_dates)].set_index('date')
            
            # è®¡ç®—æ²³é“å®½åº¦
            if 'Qout' in up_subset.columns:
                up_subset['width'] = calculate_river_width(up_subset['Qout'])
                down_subset['width'] = calculate_river_width(down_subset['Qout'])
            
            # è·å–æ²³æ®µé•¿åº¦
            length_up = get_river_length(comid, attr_dict)
            length_down = get_river_length(next_down_id, attr_dict)
            
            # è·å–æ¸©åº¦æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            temp_cols = [col for col in up_subset.columns 
                        if col in ['temperature_2m_mean', 'temperature']]
            temperature = up_subset[temp_cols[0]] if temp_cols else None
            
            # ä¸ºæ¯ä¸ªæ—¥æœŸå’Œæ¯ä¸ªå‚æ•°è®¡ç®—ä¿ç•™ç³»æ•°
            for date in common_dates:
                # print("æ­£åœ¨å¤„ç†æ—¥æœŸ:", date)
                try:
                    # è·å–å½“å¤©çš„æ•°æ®
                    up_data_day = up_subset.loc[date]
                    down_data_day = down_subset.loc[date]
                    
                    # åŸºç¡€æ•°æ®
                    Q_up = up_data_day.get('Qout', 1.0)
                    Q_down = down_data_day.get('Qout', 1.0)
                    W_up = up_data_day.get('width', 10.0)
                    W_down = down_data_day.get('width', 10.0)
                    temp_day = temperature.loc[date] if temperature is not None else 15.0
                    
                    # ä¸ºæ¯ä¸ªå‚æ•°è®¡ç®—
                    for param in parameters:
                        # æ£€æŸ¥è¯¥å‚æ•°æ˜¯å¦å·²è¾¾åˆ°è®°å½•é™åˆ¶
                        if record_counts[param] >= max_records_per_param:
                            continue
                        
                        try:
                            # é€‰æ‹©ç›¸åº”çš„å¸æ”¶é€Ÿç‡
                            v_f = v_f_TN if param == "TN" else v_f_TP
                            
                            # è·å–æ°®æµ“åº¦æ•°æ®ï¼ˆä»…å¯¹TNæœ‰æ•ˆï¼‰
                            N_concentration_day = None
                            if param == "TN" and param in up_data_day.index:
                                N_concentration_day = up_data_day[param]
                            
                            # è®¡ç®—ä¿ç•™ç³»æ•°
                            R_value = compute_retainment_factor(
                                v_f=v_f,
                                Q_up=pd.Series([Q_up]),
                                Q_down=pd.Series([Q_down]),
                                W_up=pd.Series([W_up]),
                                W_down=pd.Series([W_down]),
                                length_up=length_up,
                                length_down=length_down,
                                temperature=pd.Series([temp_day]),
                                N_concentration=pd.Series([N_concentration_day]) if N_concentration_day is not None else None,
                                parameter=param
                            )
                            
                            if hasattr(R_value, 'iloc'):
                                R_final = R_value.iloc[0]
                            else:
                                R_final = R_value
                            
                            # # åå¤„ç†ï¼šç¡®ä¿Rå€¼åœ¨åˆç†èŒƒå›´å†…
                            # if pd.isna(R_final):
                            #     R_final = 0.5
                            # R_final = max(0.0, min(1.0, R_final))
                            
                            # æ•°æ®è´¨é‡åˆ¤æ–­
                            data_quality = 'good' if all([Q_up > 0, Q_down > 0, W_up > 0, W_down > 0]) else 'poor'
                            
                            # ä¿å­˜è®°å½•
                            record = {
                                # åŸºæœ¬ä¿¡æ¯
                                'COMID': comid,
                                'NextDownID': next_down_id,
                                'date': date,
                                'parameter': param,
                                
                                # ä¸»è¦ç»“æœ
                                f'R_{param}': R_final,
                                
                                # æ ¸å¿ƒè¾“å…¥å‚æ•°
                                'v_f': v_f,
                                'Q_up_m3s': Q_up,
                                'Q_down_m3s': Q_down,
                                'W_up_m': W_up,
                                'W_down_m': W_down,
                                'length_up_km': length_up,
                                'length_down_km': length_down,
                                
                                # ç¯å¢ƒå‚æ•°
                                'temperature_C': temp_day,
                                'N_concentration_mgl': N_concentration_day if param == "TN" else None,
                                
                                # æ•°æ®è´¨é‡æ ‡å¿—
                                'data_quality_flag': data_quality
                            }
                            
                            parameter_results[param].append(record)
                            record_counts[param] += 1
                            success_counts[param] += 1
                            
                        except Exception as e:
                            logging.debug(f"è®¡ç®—å‚æ•° {param} æ—¶å‡ºé”™: {e}")
                            continue
                
                except KeyError:
                    continue  # æ—¥æœŸä¸å­˜åœ¨
                    
        except Exception as e:
            error_counts += 1
            if error_counts <= 10:  # åªè®°å½•å‰10ä¸ªé”™è¯¯
                logging.error(f"å¤„ç†COMID {comid} æ—¶å‡ºé”™: {e}")
            continue
    
    # 7. ä¿å­˜ç»“æœ
    logging.info("=" * 60)
    logging.info("ä¿å­˜è®¡ç®—ç»“æœ")
    logging.info("=" * 60)
    
    os.makedirs(output_dir, exist_ok=True)
    result_dataframes = {}
    
    for param in parameters:
        if not parameter_results[param]:
            logging.warning(f"å‚æ•° {param} æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•ä¿ç•™ç³»æ•°")
            continue
        
        # è½¬æ¢ä¸ºDataFrame
        param_df = pd.DataFrame(parameter_results[param])
        param_df = param_df.sort_values(['COMID', 'date'])
        
        # ä¿å­˜æ–‡ä»¶
        output_file = os.path.join(output_dir, f"retention_coefficients_{param}_optimized.csv")
        param_df.to_csv(output_file, index=False)
        
        result_dataframes[param] = param_df
        
        # ç»Ÿè®¡ä¿¡æ¯
        logging.info(f"\n{param} ä¿ç•™ç³»æ•°è®¡ç®—å®Œæˆ:")
        logging.info(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
        logging.info(f"  è®°å½•æ•°: {len(param_df):,}")
        logging.info(f"  æ¶‰åŠæ²³æ®µ: {param_df['COMID'].nunique():,} ä¸ª")
        
        if 'date' in param_df.columns:
            logging.info(f"  æ—¶é—´èŒƒå›´: {param_df['date'].min()} åˆ° {param_df['date'].max()}")
        
        # ä¿ç•™ç³»æ•°ç»Ÿè®¡
        r_col = f'R_{param}'
        if r_col in param_df.columns:
            r_values = param_df[r_col].dropna()
            if len(r_values) > 0:
                logging.info(f"  {param} ä¿ç•™ç³»æ•°ç»Ÿè®¡:")
                logging.info(f"    å¹³å‡å€¼: {r_values.mean():.4f}")
                logging.info(f"    æ ‡å‡†å·®: {r_values.std():.4f}")
                logging.info(f"    èŒƒå›´: {r_values.min():.4f} - {r_values.max():.4f}")
                
                # æ•°æ®è´¨é‡ç»Ÿè®¡
                good_quality = param_df[param_df['data_quality_flag'] == 'good']
                logging.info(f"    é«˜è´¨é‡æ•°æ®æ¯”ä¾‹: {len(good_quality)/len(param_df):.3f}")
    
    # 8. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    total_time = (datetime.now() - start_time).total_seconds()
    total_success = sum(success_counts.values())
    
    # åœ¨æœ€åçš„ç»Ÿè®¡ä¿¡æ¯ä¸­æ·»åŠ ï¼š
    logging.info("=" * 60)
    logging.info("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    logging.info("=" * 60)
    logging.info(f"ç»ˆç«¯æ²³æ®µæ•°é‡: {terminal_segments}")
    logging.info(f"æ•°æ®ä¸ºç©ºçš„æ²³æ®µå¯¹: {empty_data_pairs}")  
    logging.info(f"æ²¡æœ‰å…±åŒæ—¥æœŸçš„æ²³æ®µå¯¹: {no_common_dates}")
    logging.info(f"å®é™…å¤„ç†çš„æ²³æ®µå¯¹: {processed_pairs}")

    logging.info("=" * 60)
    logging.info("æ€§èƒ½ç»Ÿè®¡")
    logging.info("=" * 60)
    logging.info(f"æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    logging.info(f"å¤„ç†çš„COMIDæ•°: {len(processable_comids):,}")
    logging.info(f"æˆåŠŸè®¡ç®—çš„è®°å½•æ•°: {total_success:,}")
    logging.info(f"é”™è¯¯æ•°: {error_counts}")
    logging.info(f"å¹³å‡å¤„ç†é€Ÿåº¦: {len(processable_comids)/total_time:.1f} COMID/ç§’")
    
    if total_success > 0:
        logging.info(f"å¹³å‡æ¯è®°å½•ç”¨æ—¶: {total_time/total_success*1000:.2f} æ¯«ç§’")
    
    return result_dataframes


def create_performance_summary(
    result_dataframes: Dict,
    output_dir: str,
    total_time_seconds: float,
    processed_comids: int
):
    """
    åˆ›å»ºæ€§èƒ½å’Œç»“æœæ€»ç»“æŠ¥å‘Š
    """
    
    summary_file = Path(output_dir) / "performance_summary.txt"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ä¼˜åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®— - æ€§èƒ½å’Œç»“æœæ€»ç»“\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("1. æ€§èƒ½ç»Ÿè®¡\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ€»è¿è¡Œæ—¶é—´: {total_time_seconds/60:.1f} åˆ†é’Ÿ\n")
        f.write(f"å¤„ç†COMIDæ•°: {processed_comids:,}\n")
        f.write(f"å¹³å‡å¤„ç†é€Ÿåº¦: {processed_comids/total_time_seconds:.1f} COMID/ç§’\n")
        f.write(f"é¢„æœŸæ€§èƒ½æå‡: 10-50å€ï¼ˆç›¸æ¯”åŸå§‹æ–¹æ³•ï¼‰\n\n")
        
        f.write("2. è®¡ç®—ç»“æœ\n")
        f.write("-" * 40 + "\n")
        
        total_records = 0
        for param, df in result_dataframes.items():
            if df is not None and not df.empty:
                f.write(f"{param} å‚æ•°:\n")
                f.write(f"  è®°å½•æ•°: {len(df):,}\n")
                f.write(f"  æ¶‰åŠæ²³æ®µ: {df['COMID'].nunique():,}\n")
                
                r_col = f'R_{param}'
                if r_col in df.columns:
                    r_values = df[r_col].dropna()
                    f.write(f"  ä¿ç•™ç³»æ•°å¹³å‡å€¼: {r_values.mean():.4f}\n")
                    f.write(f"  ä¿ç•™ç³»æ•°æ ‡å‡†å·®: {r_values.std():.4f}\n")
                
                total_records += len(df)
                f.write("\n")
        
        f.write(f"æ€»è®°å½•æ•°: {total_records:,}\n\n")
        
        f.write("3. ä¼˜åŒ–æ•ˆæœ\n")
        f.write("-" * 40 + "\n")
        f.write("æ•°æ®è®¿é—®å¤æ‚åº¦: O(MÃ—N) â†’ O(1)\n")
        f.write("å†…å­˜ä½¿ç”¨: å¤§å¹…å‡å°‘\n")
        f.write("å¯æ‰©å±•æ€§: æ˜¾è‘—æå‡\n")
        f.write("å¹¶è¡ŒåŒ–å‹å¥½: æ˜¯\n\n")
        
        f.write("4. æ–‡ä»¶è¾“å‡º\n")
        f.write("-" * 40 + "\n")
        for param in result_dataframes.keys():
            f.write(f"retention_coefficients_{param}_optimized.csv\n")
    
    logging.info(f"æ€§èƒ½æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œä¼˜åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—
    """
    
    # é…ç½®å‚æ•°
    SPLIT_DATA_DIR = "split_data"  # æ‹†åˆ†æ•°æ®ç›®å½•
    ATTR_DATA_PATH = "data/river_attributes_new.csv"  # æ²³æ®µå±æ€§æ•°æ®
    OUTPUT_DIR = "output_optimized"
    PARAMETERS = ["TN", "TP"]
    V_F_TN = 35.0
    V_F_TP = 44.5
    MAX_RECORDS_PER_PARAM = 100000000
    
    print("ğŸš€ ä¼˜åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—ç¨‹åºå¯åŠ¨")
    print("=" * 60)
    print(f"æ‹†åˆ†æ•°æ®ç›®å½•: {SPLIT_DATA_DIR}")
    print(f"æ²³æ®µå±æ€§æ–‡ä»¶: {ATTR_DATA_PATH}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"è®¡ç®—å‚æ•°: {PARAMETERS}")
    print("=" * 60)
    
    try:
        start_time = datetime.now()
        
        # æ‰§è¡Œè®¡ç®—
        result_dataframes = calculate_retention_coefficients_optimized(
            split_data_dir=SPLIT_DATA_DIR,
            attr_data_path=ATTR_DATA_PATH,
            output_dir=OUTPUT_DIR,
            parameters=PARAMETERS,
            v_f_TN=V_F_TN,
            v_f_TP=V_F_TP,
            max_records_per_param=MAX_RECORDS_PER_PARAM
        )
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ç»Ÿè®¡å¤„ç†çš„COMIDæ•°
        total_comids = 0
        if result_dataframes:
            sample_df = next(iter(result_dataframes.values()))
            if not sample_df.empty:
                total_comids = sample_df['COMID'].nunique()
        
        # åˆ›å»ºæ€§èƒ½æ€»ç»“
        create_performance_summary(
            result_dataframes=result_dataframes,
            output_dir=OUTPUT_DIR,
            total_time_seconds=total_time,
            processed_comids=total_comids
        )
        
        print("\nğŸ‰ ä¼˜åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—å®Œæˆï¼")
        print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š å¤„ç†COMIDæ•°: {total_comids:,}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
        print(f"ğŸ“ˆ æ€§èƒ½æ€»ç»“: {OUTPUT_DIR}/performance_summary.txt")
        
        # ä¸åŸå§‹æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”æç¤º
        print("\nğŸ’¡ æ€§èƒ½æå‡æ•ˆæœ:")
        print("   - é¢„æœŸè¿è¡Œæ—¶é—´å‡å°‘: 80-95%")
        print("   - å†…å­˜ä½¿ç”¨å‡å°‘: 70-90%")
        print("   - æ•°æ®è®¿é—®å¤æ‚åº¦: O(MÃ—N) â†’ O(1)")
        
        
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)