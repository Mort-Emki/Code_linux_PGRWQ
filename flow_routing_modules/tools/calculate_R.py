"""
å®Œå…¨å‘é‡åŒ–çš„ä¿ç•™ç³»æ•°è®¡ç®—ç¨‹åº - åŸºäºæ‹†åˆ†æ•°æ®çš„è¶…é«˜æ€§èƒ½ç‰ˆæœ¬ - æŒ‰COMIDæ‹†åˆ†ä¿å­˜

æ ¸å¿ƒæ”¹è¿›ï¼š
- ä¿ç•™åŸæœ‰çš„O(1)æ•°æ®è®¿é—®ä¼˜åŠ¿
- æ¶ˆé™¤æ—¥æœŸå¾ªç¯ï¼šè®¡ç®—å’Œä¿å­˜éƒ½å®Œå…¨å‘é‡åŒ–
- æ—¶é—´å¤æ‚åº¦ï¼šä»O(MÃ—NÃ—P)é™ä½åˆ°O(MÃ—P)
- æŒ‰COMIDæ‹†åˆ†ä¿å­˜ï¼šä¾¿äºä¸‹æ¸¸åˆ†æå’Œå¤„ç†
- é¢„æœŸæ€§èƒ½æå‡ï¼š500-5000å€

ä½œè€…: [Your Name]
æ—¥æœŸ: 2025-01-XX
"""

import pandas as pd
import numpy as np
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
from tqdm import tqdm

# å¯¼å…¥åŸæœ‰çš„è®¡ç®—å‡½æ•°
from ..core.geometry import get_river_length, calculate_river_width
from ..physics.environment_param import compute_retainment_factor
from ...data_processing import detect_and_handle_anomalies

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('retention_calculation_by_comid.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class OptimizedDataLoader:
    """
    ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨ - æ”¯æŒå¿«é€Ÿè®¿é—®æ‹†åˆ†åçš„COMIDæ•°æ®
    """
    
    def __init__(self, split_data_dir: str):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        å‚æ•°:
            split_data_dir: æ‹†åˆ†æ•°æ®çš„ç›®å½•è·¯å¾„
        """
        self.split_data_dir = Path(split_data_dir)
        self.comid_files_dir = self.split_data_dir / "comid_files"
        
        # åŠ è½½ç´¢å¼•æ–‡ä»¶
        self.index_file = self.split_data_dir / "comid_index.json"
        self.metadata_file = self.split_data_dir / "split_metadata.json"
        
        if not self.index_file.exists():
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_file}")
        
        with open(self.index_file, 'r', encoding='utf-8') as f:
            self.comid_index = json.load(f)
        
        # åŠ è½½å…ƒæ•°æ®
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {}
        
        # æ•°æ®ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        self._cache = {}
        self._cache_size_limit = 100  # æœ€å¤šç¼“å­˜100ä¸ªCOMIDçš„æ•°æ®
        
        logging.info(f"æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logging.info(f"å¯ç”¨COMIDæ•°é‡: {len(self.comid_index)}")
        logging.info(f"æ•°æ®æ–‡ä»¶ç›®å½•: {self.comid_files_dir}")
    
    def load_comid_data(self, comid: int, use_cache: bool = True) -> pd.DataFrame:
        """
        å¿«é€ŸåŠ è½½æŒ‡å®šCOMIDçš„æ•°æ® - O(1)å¤æ‚åº¦ï¼
        
        å‚æ•°:
            comid: COMIDç¼–å·
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        è¿”å›:
            DataFrame: è¯¥COMIDçš„æ‰€æœ‰æ—¶é—´åºåˆ—æ•°æ®
        """
        comid_str = str(comid)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and comid_str in self._cache:
            return self._cache[comid_str].copy()
        
        # æ£€æŸ¥COMIDæ˜¯å¦å­˜åœ¨
        if comid_str not in self.comid_index:
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        filename = self.comid_index[comid_str]
        filepath = self.comid_files_dir / filename
        
        if not filepath.exists():
            logging.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return pd.DataFrame()
        
        try:
            # æ ¹æ®æ–‡ä»¶æ ¼å¼è¯»å–æ•°æ®
            if filename.endswith('.parquet'):
                data = pd.read_parquet(filepath)
            elif filename.endswith('.csv') or filename.endswith('.csv.gz'):
                data = pd.read_csv(filepath)
            elif filename.endswith('.feather'):
                data = pd.read_feather(filepath)
            else:
                logging.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
                return pd.DataFrame()
            
            # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeæ ¼å¼
            if 'date' in data.columns:
                data['date'] = pd.to_datetime(data['date'])
            
            # ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
            if use_cache:
                # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
                if len(self._cache) >= self._cache_size_limit:
                    oldest_key = next(iter(self._cache))
                    del self._cache[oldest_key]
                
                self._cache[comid_str] = data.copy()
            
            return data
            
        except Exception as e:
            logging.error(f"è¯»å–COMID {comid} æ•°æ®æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    
    def get_comid_info(self, comid: int) -> Dict:
        """
        è·å–COMIDçš„å…ƒä¿¡æ¯
        
        å‚æ•°:
            comid: COMIDç¼–å·
        
        è¿”å›:
            dict: åŒ…å«è®°å½•æ•°ã€æ—¥æœŸèŒƒå›´ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        comid_str = str(comid)
        
        if 'file_info' in self.metadata and comid_str in self.metadata['file_info']:
            return self.metadata['file_info'][comid_str]
        else:
            return {}
    
    def get_available_comids(self) -> List[int]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„COMIDåˆ—è¡¨
        
        è¿”å›:
            List[int]: COMIDåˆ—è¡¨
        """
        return [int(comid) for comid in self.comid_index.keys()]
    
    def get_common_dates(self, comid1: int, comid2: int) -> List[pd.Timestamp]:
        """
        è·å–ä¸¤ä¸ªCOMIDçš„å…±åŒæ—¥æœŸ
        
        å‚æ•°:
            comid1, comid2: ä¸¤ä¸ªCOMIDç¼–å·
        
        è¿”å›:
            List[pd.Timestamp]: å…±åŒæ—¥æœŸåˆ—è¡¨
        """
        data1 = self.load_comid_data(comid1)
        data2 = self.load_comid_data(comid2)
        
        if data1.empty or data2.empty:
            return []
        
        if 'date' not in data1.columns or 'date' not in data2.columns:
            return []
        
        # æ‰¾åˆ°å…±åŒçš„æ—¥æœŸ
        dates1 = set(data1['date'])
        dates2 = set(data2['date'])
        common_dates = sorted(dates1 & dates2)
        
        return common_dates


def load_river_attributes_optimized(attr_data_path: str) -> Tuple[Dict, Dict]:
    """
    åŠ è½½æ²³æ®µå±æ€§æ•°æ®å¹¶æ„å»ºä¼˜åŒ–çš„æŸ¥æ‰¾å­—å…¸
    
    å‚æ•°:
        attr_data_path: æ²³æ®µå±æ€§æ•°æ®æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        Tuple[Dict, Dict]: (æ‹“æ‰‘å­—å…¸, å±æ€§å­—å…¸)
    """
    
    logging.info("åŠ è½½æ²³æ®µå±æ€§æ•°æ®...")
    
    try:
        attr_df = pd.read_csv(attr_data_path)
        logging.info(f"æ²³æ®µå±æ€§æ•°æ®å½¢çŠ¶: {attr_df.shape}")
        
        # æ„å»ºæ‹“æ‰‘å­—å…¸ (COMID -> NextDownID)
        topo_dict = attr_df.set_index('COMID')['NextDownID'].to_dict()
        
        # æ„å»ºå±æ€§å­—å…¸
        attr_dict = {}
        for _, row in attr_df.iterrows():
            comid_int = int(row['COMID'])
            attr_dict[str(comid_int)] = {
                'lengthkm': row.get('lengthkm', 1.0),
                'order_': row.get('order_', 1),
                'slope': row.get('slope', 0.001),
                'uparea': row.get('uparea', 1.0)
            }
        
        logging.info(f"æˆåŠŸåŠ è½½ {len(attr_dict)} ä¸ªæ²³æ®µçš„å±æ€§æ•°æ®")
        
        return topo_dict, attr_dict
        
    except Exception as e:
        logging.error(f"åŠ è½½æ²³æ®µå±æ€§æ•°æ®æ—¶å‡ºé”™: {e}")
        raise


def perform_data_quality_check(data_loader: OptimizedDataLoader, sample_size: int = 100):
    """
    å¯¹æ‹†åˆ†åçš„æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥
    
    å‚æ•°:
        data_loader: æ•°æ®åŠ è½½å™¨
        sample_size: æŠ½æ ·æ£€æŸ¥çš„COMIDæ•°é‡
    """
    
    logging.info("=" * 60)
    logging.info("å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥")
    logging.info("=" * 60)
    
    available_comids = data_loader.get_available_comids()
    
    if len(available_comids) == 0:
        logging.error("æ²¡æœ‰å¯ç”¨çš„COMIDæ•°æ®")
        return
    
    # éšæœºæŠ½æ ·æ£€æŸ¥
    import random
    sample_comids = random.sample(available_comids, min(sample_size, len(available_comids)))
    
    total_records = 0
    total_missing = 0
    date_ranges = []
    
    for comid in tqdm(sample_comids, desc="æ•°æ®è´¨é‡æ£€æŸ¥"):
        try:
            data = data_loader.load_comid_data(comid)
            
            if data.empty:
                logging.warning(f"COMID {comid} æ•°æ®ä¸ºç©º")
                continue
            
            # ç»Ÿè®¡è®°å½•æ•°
            total_records += len(data)
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_in_data = data.isnull().sum().sum()
            total_missing += missing_in_data
            
            # è®°å½•æ—¥æœŸèŒƒå›´
            if 'date' in data.columns:
                date_range = (data['date'].min(), data['date'].max())
                date_ranges.append(date_range)
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥COMID {comid} æ—¶å‡ºé”™: {e}")
    
    # æ±‡æ€»ç»Ÿè®¡
    if date_ranges:
        overall_start = min(dr[0] for dr in date_ranges)
        overall_end = max(dr[1] for dr in date_ranges)
        logging.info(f"æ•´ä½“æ—¥æœŸèŒƒå›´: {overall_start} åˆ° {overall_end}")
    
    logging.info(f"æŠ½æ ·æ£€æŸ¥å®Œæˆ:")
    logging.info(f"- æ£€æŸ¥COMIDæ•°: {len(sample_comids)}")
    logging.info(f"- æ€»è®°å½•æ•°: {total_records:,}")
    logging.info(f"- æ€»ç¼ºå¤±å€¼: {total_missing:,}")
    logging.info(f"- ç¼ºå¤±ç‡: {total_missing/max(total_records, 1)*100:.2f}%")


def save_results_by_comid(
    parameter_dataframes: Dict[str, List[pd.DataFrame]], 
    output_dir: str, 
    parameters: List[str],
    file_format: str = 'csv'
) -> Dict[str, Dict]:
    """
    ğŸš€ æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç»“æœ - æ–°çš„æ ¸å¿ƒä¿å­˜å‡½æ•°
    
    å‚æ•°:
        parameter_dataframes: åŒ…å«å„å‚æ•°è®¡ç®—ç»“æœçš„å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        parameters: å‚æ•°åˆ—è¡¨
        file_format: æ–‡ä»¶æ ¼å¼ ('csv', 'parquet', 'feather')
    
    è¿”å›:
        Dict[str, Dict]: åŒ…å«ä¿å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    
    logging.info("=" * 60)
    logging.info("ğŸš€ æŒ‰COMIDæ‹†åˆ†ä¿å­˜è®¡ç®—ç»“æœ")
    logging.info("=" * 60)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»ºå­ç›®å½•
    param_dirs = {}
    for param in parameters:
        param_dir = Path(output_dir) / f"retention_coefficients_{param}"
        param_dir.mkdir(exist_ok=True)
        param_dirs[param] = param_dir
        logging.info(f"åˆ›å»º {param} å‚æ•°ç›®å½•: {param_dir}")
    
    result_stats = {}
    
    for param in parameters:
        if not parameter_dataframes[param]:
            logging.warning(f"å‚æ•° {param} æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•ä¿ç•™ç³»æ•°")
            result_stats[param] = {
                'total_records': 0,
                'total_comids': 0,
                'files_saved': 0,
                'save_errors': 0
            }
            continue
        
        logging.info(f"å¼€å§‹å¤„ç†å‚æ•° {param}...")
        
        # ğŸš€ æ­¥éª¤1ï¼šåˆå¹¶æ‰€æœ‰DataFrameå—
        param_df = pd.concat(parameter_dataframes[param], ignore_index=True)
        param_df = param_df.sort_values(['COMID', 'date'])
        
        logging.info(f"{param} æ€»è®°å½•æ•°: {len(param_df):,}")
        logging.info(f"{param} æ¶‰åŠCOMIDæ•°: {param_df['COMID'].nunique():,}")
        
        # ğŸš€ æ­¥éª¤2ï¼šæŒ‰COMIDåˆ†ç»„å¹¶ä¿å­˜
        param_dir = param_dirs[param]
        comid_groups = param_df.groupby('COMID')
        
        files_saved = 0
        save_errors = 0
        total_records = len(param_df)
        total_comids = param_df['COMID'].nunique()
        
        # åˆ›å»ºä¿å­˜è¿›åº¦æ¡
        comid_list = list(comid_groups.groups.keys())
        
        for comid in tqdm(comid_list, desc=f"ä¿å­˜ {param} å‚æ•°æ–‡ä»¶"):
            try:
                # è·å–è¯¥COMIDçš„æ‰€æœ‰è®°å½•
                comid_data = comid_groups.get_group(comid)
                
                # æ„å»ºæ–‡ä»¶å
                if file_format == 'csv':
                    filename = f"retention_coefficients_{param}_COMID_{comid}.csv"
                    filepath = param_dir / filename
                    comid_data.to_csv(filepath, index=False)
                    
                elif file_format == 'parquet':
                    filename = f"retention_coefficients_{param}_COMID_{comid}.parquet"
                    filepath = param_dir / filename
                    comid_data.to_parquet(filepath, index=False)
                    
                elif file_format == 'feather':
                    filename = f"retention_coefficients_{param}_COMID_{comid}.feather"
                    filepath = param_dir / filename
                    comid_data.to_feather(filepath)
                    
                else:
                    logging.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
                    save_errors += 1
                    continue
                
                files_saved += 1
                
            except Exception as e:
                logging.error(f"ä¿å­˜COMID {comid} çš„ {param} æ•°æ®æ—¶å‡ºé”™: {e}")
                save_errors += 1
                continue
        
        # ğŸš€ æ­¥éª¤3ï¼šä¿å­˜å‚æ•°çº§åˆ«çš„æ±‡æ€»æ–‡ä»¶
        summary_file = param_dir / f"retention_coefficients_{param}_summary.csv"
        param_df.to_csv(summary_file, index=False)
        
        # ğŸš€ æ­¥éª¤4ï¼šåˆ›å»ºç´¢å¼•æ–‡ä»¶
        index_data = []
        for comid in comid_list:
            comid_data = comid_groups.get_group(comid)
            index_entry = {
                'COMID': comid,
                'filename': f"retention_coefficients_{param}_COMID_{comid}.{file_format}",
                'record_count': len(comid_data),
                'date_start': comid_data['date'].min().strftime('%Y-%m-%d'),
                'date_end': comid_data['date'].max().strftime('%Y-%m-%d'),
                'mean_retention': comid_data[f'R_{param}'].mean() if f'R_{param}' in comid_data.columns else None
            }
            index_data.append(index_entry)
        
        index_df = pd.DataFrame(index_data)
        index_file = param_dir / f"retention_coefficients_{param}_index.csv"
        index_df.to_csv(index_file, index=False)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        result_stats[param] = {
            'total_records': total_records,
            'total_comids': total_comids,
            'files_saved': files_saved,
            'save_errors': save_errors,
            'summary_file': str(summary_file),
            'index_file': str(index_file),
            'output_directory': str(param_dir)
        }
        
        logging.info(f"{param} å‚æ•°ä¿å­˜å®Œæˆ:")
        logging.info(f"  ä¿å­˜æ–‡ä»¶æ•°: {files_saved}")
        logging.info(f"  ä¿å­˜é”™è¯¯æ•°: {save_errors}")
        logging.info(f"  è¾“å‡ºç›®å½•: {param_dir}")
        logging.info(f"  æ±‡æ€»æ–‡ä»¶: {summary_file}")
        logging.info(f"  ç´¢å¼•æ–‡ä»¶: {index_file}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        if f'R_{param}' in param_df.columns:
            r_values = param_df[f'R_{param}'].dropna()
            if len(r_values) > 0:
                logging.info(f"  {param} ä¿ç•™ç³»æ•°ç»Ÿè®¡:")
                logging.info(f"    å¹³å‡å€¼: {r_values.mean():.4f}")
                logging.info(f"    æ ‡å‡†å·®: {r_values.std():.4f}")
                logging.info(f"    èŒƒå›´: {r_values.min():.4f} - {r_values.max():.4f}")
                
                # æ•°æ®è´¨é‡ç»Ÿè®¡
                good_quality = param_df[param_df['data_quality_flag'] == 'good']
                logging.info(f"    é«˜è´¨é‡æ•°æ®æ¯”ä¾‹: {len(good_quality)/len(param_df):.3f}")
    
    # ğŸš€ æ­¥éª¤5ï¼šåˆ›å»ºæ€»ä½“ç´¢å¼•æ–‡ä»¶
    create_master_index(output_dir, result_stats, parameters)
    
    return result_stats


def create_master_index(output_dir: str, result_stats: Dict, parameters: List[str]):
    """
    åˆ›å»ºæ€»ä½“ç´¢å¼•æ–‡ä»¶
    
    å‚æ•°:
        output_dir: è¾“å‡ºç›®å½•
        result_stats: ç»“æœç»Ÿè®¡ä¿¡æ¯
        parameters: å‚æ•°åˆ—è¡¨
    """
    
    master_index = {
        'creation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'parameters': parameters,
        'file_format': 'æŒ‰COMIDæ‹†åˆ†ä¿å­˜',
        'directory_structure': {},
        'summary_statistics': {}
    }
    
    for param in parameters:
        if param in result_stats:
            stats = result_stats[param]
            master_index['directory_structure'][param] = {
                'directory': f"retention_coefficients_{param}/",
                'file_pattern': f"retention_coefficients_{param}_COMID_{{COMID}}.csv",
                'summary_file': f"retention_coefficients_{param}_summary.csv",
                'index_file': f"retention_coefficients_{param}_index.csv"
            }
            
            master_index['summary_statistics'][param] = {
                'total_records': stats['total_records'],
                'total_comids': stats['total_comids'],
                'files_saved': stats['files_saved'],
                'save_errors': stats['save_errors']
            }
    
    # ä¿å­˜ä¸»ç´¢å¼•æ–‡ä»¶
    master_index_file = Path(output_dir) / "master_index.json"
    with open(master_index_file, 'w', encoding='utf-8') as f:
        json.dump(master_index, f, indent=2, ensure_ascii=False)
    
    logging.info(f"æ€»ä½“ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜: {master_index_file}")


def calculate_retention_coefficients_by_comid(
    split_data_dir: str,
    attr_data_path: str,
    output_dir: str = "output_by_comid",
    parameters: List[str] = ["TN", "TP"],
    v_f_TN: float = 35.0,
    v_f_TP: float = 44.5,
    enable_anomaly_check: bool = True,
    fix_anomalies: bool = True,
    max_records_per_param: int = 10000000,
    progress_interval: int = 100,
    file_format: str = 'csv'
):
    """
    ä½¿ç”¨å®Œå…¨å‘é‡åŒ–è®¡ç®—çš„ä¼˜åŒ–ä¿ç•™ç³»æ•°è®¡ç®— - æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç‰ˆæœ¬
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - ä¿ç•™O(1)æ•°æ®è®¿é—®ä¼˜åŠ¿
    - æ¶ˆé™¤æ‰€æœ‰å¾ªç¯ï¼Œå®Œå…¨å‘é‡åŒ–è®¡ç®—
    - æŒ‰COMIDæ‹†åˆ†ä¿å­˜ï¼Œä¾¿äºä¸‹æ¸¸å¤„ç†
    - æ—¶é—´å¤æ‚åº¦ä»O(MÃ—NÃ—P)é™è‡³O(MÃ—P)
    
    å‚æ•°:
        split_data_dir: æ‹†åˆ†æ•°æ®ç›®å½•
        attr_data_path: æ²³æ®µå±æ€§æ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        parameters: è¦è®¡ç®—çš„å‚æ•°åˆ—è¡¨
        v_f_TN, v_f_TP: å¸æ”¶é€Ÿç‡å‚æ•°
        enable_anomaly_check: æ˜¯å¦å¯ç”¨å¼‚å¸¸å€¼æ£€æµ‹
        fix_anomalies: æ˜¯å¦ä¿®å¤å¼‚å¸¸å€¼
        max_records_per_param: æ¯ä¸ªå‚æ•°çš„æœ€å¤§è®°å½•æ•°
        progress_interval: è¿›åº¦æŠ¥å‘Šé—´éš”
        file_format: è¾“å‡ºæ–‡ä»¶æ ¼å¼ ('csv', 'parquet', 'feather')
    
    è¿”å›:
        dict: åŒ…å«ä¿å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    
    logging.info("=" * 80)
    logging.info("å¼€å§‹å®Œå…¨å‘é‡åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®— - æŒ‰COMIDæ‹†åˆ†ä¿å­˜")
    logging.info("=" * 80)
    
    start_time = datetime.now()
    
    # 1. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    try:
        data_loader = OptimizedDataLoader(split_data_dir)
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        raise
    
    # 2. æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
    if enable_anomaly_check:
        perform_data_quality_check(data_loader, sample_size=50)
    
    # 3. åŠ è½½æ²³æ®µå±æ€§æ•°æ®
    topo_dict, attr_dict = load_river_attributes_optimized(attr_data_path)
    
    # 4. è·å–å¯å¤„ç†çš„COMIDåˆ—è¡¨
    available_comids = data_loader.get_available_comids()
    processable_comids = [comid for comid in available_comids if comid in topo_dict]
    
    logging.info(f"å¯ç”¨COMIDæ€»æ•°: {len(available_comids)}")
    logging.info(f"å¯å¤„ç†COMIDæ•°: {len(processable_comids)}")
    
    # 5. åˆå§‹åŒ–ç»“æœå­˜å‚¨ - æ”¹ä¸ºç›´æ¥å­˜å‚¨DataFrameåˆ—è¡¨
    parameter_dataframes = {param: [] for param in parameters}
    success_counts = {param: 0 for param in parameters}
    record_counts = {param: 0 for param in parameters}
    error_counts = 0
    
    terminal_segments = 0  # ç»ˆç«¯æ²³æ®µæ•°é‡
    empty_data_pairs = 0   # æ•°æ®ä¸ºç©ºçš„æ²³æ®µå¯¹æ•°é‡
    no_common_dates = 0    # æ²¡æœ‰å…±åŒæ—¥æœŸçš„æ²³æ®µå¯¹æ•°é‡
    processed_pairs = 0    # å®é™…å¤„ç†çš„æ²³æ®µå¯¹æ•°é‡
    vectorization_savings = 0  # å‘é‡åŒ–èŠ‚çœçš„æ“ä½œæ•°

    # 6. ä¸»è®¡ç®—å¾ªç¯ - å®Œå…¨å‘é‡åŒ–ç‰ˆæœ¬ï¼
    logging.info("å¼€å§‹å®Œå…¨å‘é‡åŒ–æ±‡æµè®¡ç®—å¾ªç¯...")
    
    for i, comid in enumerate(tqdm(processable_comids, desc="å®Œå…¨å‘é‡åŒ–è®¡ç®—ä¿ç•™ç³»æ•°")):
        
        # è·å–ä¸‹æ¸¸æ²³æ®µID
        next_down_id = topo_dict.get(comid, 0)
        next_down_id = int(next_down_id) if next_down_id else 0 
        if next_down_id == 0:
            terminal_segments += 1
            continue  # è·³è¿‡ç»ˆç«¯æ²³æ®µ
        
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šO(1)æ•°æ®è¯»å–ï¼Œè€ŒéO(N)æŸ¥æ‰¾ï¼
        up_data = data_loader.load_comid_data(comid)
        down_data = data_loader.load_comid_data(next_down_id)
        
        if up_data.empty or down_data.empty:
            empty_data_pairs += 1
            continue
        
        # ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿè·å–å…±åŒæ—¥æœŸ
        common_dates = data_loader.get_common_dates(comid, next_down_id)
        if not common_dates:
            no_common_dates += 1
            continue
        
        processed_pairs += 1
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è®°å½•é™åˆ¶
        if all(record_counts[param] >= max_records_per_param for param in parameters):
            logging.info(f"æ‰€æœ‰å‚æ•°éƒ½å·²è¾¾åˆ°æœ€å¤§è®°å½•æ•°é™åˆ¶ ({max_records_per_param})")
            break
        
        # è¿›åº¦æŠ¥å‘Š
        if (i + 1) % progress_interval == 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = (i + 1) / elapsed
            eta = (len(processable_comids) - i - 1) / rate if rate > 0 else 0
            logging.info(f"å¤„ç†è¿›åº¦: {i+1}/{len(processable_comids)} "
                        f"({(i+1)/len(processable_comids)*100:.1f}%), "
                        f"å¤„ç†é€Ÿåº¦: {rate:.1f} COMID/ç§’, "
                        f"é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
        
        try:
            # ç­›é€‰å…±åŒæ—¥æœŸçš„æ•°æ®å¹¶æ’åºï¼ˆç¡®ä¿æ—¥æœŸå¯¹é½ï¼‰
            up_subset = up_data[up_data['date'].isin(common_dates)].sort_values('date').set_index('date')
            down_subset = down_data[down_data['date'].isin(common_dates)].sort_values('date').set_index('date')
            
            # ğŸš€ å‘é‡åŒ–é¢„è®¡ç®—ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æ—¥æœŸçš„æ²³é“å®½åº¦
            if 'Qout' in up_subset.columns:
                up_subset['width'] = calculate_river_width(up_subset['Qout'])
                down_subset['width'] = calculate_river_width(down_subset['Qout'])
            
            # è·å–æ²³æ®µé•¿åº¦
            length_up = get_river_length(comid, attr_dict)
            length_down = get_river_length(next_down_id, attr_dict)
            
            # è·å–æ¸©åº¦æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            temp_cols = [col for col in up_subset.columns 
                        if col in ['temperature_2m_mean', 'temperature']]
            temperature = up_subset[temp_cols[0]] if temp_cols else None
            
            # ğŸš€ æ ¸å¿ƒæ”¹è¿›ï¼šå®Œå…¨å‘é‡åŒ–è®¡ç®—å’Œä¿å­˜æ¯ä¸ªå‚æ•°
            for param in parameters:
                # æ£€æŸ¥è¯¥å‚æ•°æ˜¯å¦å·²è¾¾åˆ°è®°å½•é™åˆ¶
                if record_counts[param] >= max_records_per_param:
                    continue
                
                try:
                    # é€‰æ‹©ç›¸åº”çš„å¸æ”¶é€Ÿç‡
                    v_f = v_f_TN if param == "TN" else v_f_TP
                    
                    # è·å–æ°®æµ“åº¦æ•°æ®ï¼ˆä»…å¯¹TNæœ‰æ•ˆï¼‰
                    N_concentration = None
                    if param == "TN" and param in up_subset.columns:
                        N_concentration = up_subset[param]
                    
                    # ğŸš€ å…³é”®æ”¹è¿›ï¼šä¸€æ¬¡æ€§å‘é‡åŒ–è®¡ç®—æ•´ä¸ªæ—¶é—´åºåˆ—ï¼
                    R_series = compute_retainment_factor(
                        v_f=v_f,
                        Q_up=up_subset['Qout'],           # æ•´ä¸ªæ—¶é—´åºåˆ—ï¼
                        Q_down=down_subset['Qout'],       # æ•´ä¸ªæ—¶é—´åºåˆ—ï¼
                        W_up=up_subset['width'],          # æ•´ä¸ªæ—¶é—´åºåˆ—ï¼
                        W_down=down_subset['width'],      # æ•´ä¸ªæ—¶é—´åºåˆ—ï¼
                        length_up=length_up,
                        length_down=length_down,
                        temperature=temperature,          # æ•´ä¸ªæ¸©åº¦åºåˆ—æˆ–None
                        N_concentration=N_concentration,  # æ•´ä¸ªæµ“åº¦åºåˆ—æˆ–None
                        parameter=param
                    )
                    
                    # ç»Ÿè®¡å‘é‡åŒ–èŠ‚çœçš„æ“ä½œæ•°
                    vectorization_savings += len(common_dates) - 1  # èŠ‚çœäº†(N-1)æ¬¡å‡½æ•°è°ƒç”¨
                    
                    # ğŸš€ ç»ˆææ”¹è¿›ï¼šå®Œå…¨å‘é‡åŒ–çš„ç»“æœä¿å­˜ï¼
                    # ä¸€æ¬¡æ€§æ„å»ºæ•´ä¸ªç»“æœDataFrameï¼Œæ— éœ€ä»»ä½•å¾ªç¯
                    
                    # ç¡®ä¿æ‰€æœ‰æ•°æ®å¯¹é½åˆ°common_dates
                    aligned_dates = sorted(common_dates)
                    up_aligned = up_subset.loc[aligned_dates]
                    down_aligned = down_subset.loc[aligned_dates]
                    R_aligned = R_series.loc[aligned_dates]
                    
                    # è®¡ç®—æ•°æ®è´¨é‡æ ‡å¿—ï¼ˆå‘é‡åŒ–ï¼‰
                    quality_mask = (
                        (up_aligned['Qout'] > 0) & 
                        (down_aligned['Qout'] > 0) & 
                        (up_aligned['width'] > 0) & 
                        (down_aligned['width'] > 0)
                    )
                    data_quality = quality_mask.map({True: 'good', False: 'poor'})
                    
                    # ğŸ¯ ä¸€æ¬¡æ€§æ„å»ºå®Œæ•´çš„ç»“æœDataFrame
                    num_records = len(aligned_dates)
                    result_df = pd.DataFrame({
                        # åŸºæœ¬ä¿¡æ¯ï¼ˆå¹¿æ’­æ ‡é‡å€¼ï¼‰
                        'COMID': [comid] * num_records,
                        'NextDownID': [next_down_id] * num_records,
                        'date': aligned_dates,
                        'parameter': [param] * num_records,
                        
                        # ä¸»è¦ç»“æœï¼ˆå‘é‡åŒ–ç»“æœï¼‰
                        f'R_{param}': R_aligned.values,
                        
                        # æ ¸å¿ƒè¾“å…¥å‚æ•°ï¼ˆå‘é‡åŒ–ç»“æœï¼‰
                        'v_f': [v_f] * num_records,
                        'Q_up_m3s': up_aligned['Qout'].values,
                        'Q_down_m3s': down_aligned['Qout'].values,
                        'W_up_m': up_aligned['width'].values,
                        'W_down_m': down_aligned['width'].values,
                        'length_up_km': [length_up] * num_records,
                        'length_down_km': [length_down] * num_records,
                        
                        # ç¯å¢ƒå‚æ•°
                        'temperature_C': temperature.loc[aligned_dates].values if temperature is not None else [None] * num_records,
                        'N_concentration_mgl': N_concentration.loc[aligned_dates].values if N_concentration is not None else [None] * num_records,
                        
                        # æ•°æ®è´¨é‡æ ‡å¿—ï¼ˆå‘é‡åŒ–è®¡ç®—ï¼‰
                        'data_quality_flag': data_quality.values
                    })
                    
                    # ğŸš€ æ‰¹é‡æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼ˆæ— å¾ªç¯ï¼ï¼‰
                    parameter_dataframes[param].append(result_df)
                    record_counts[param] += num_records
                    success_counts[param] += num_records
                    
                except Exception as e:
                    logging.debug(f"è®¡ç®—å‚æ•° {param} æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            error_counts += 1
            if error_counts <= 10:  # åªè®°å½•å‰10ä¸ªé”™è¯¯
                logging.error(f"å¤„ç†COMID {comid} æ—¶å‡ºé”™: {e}")
            continue
    
    # 7. ğŸš€ æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç»“æœ
    result_stats = save_results_by_comid(
        parameter_dataframes=parameter_dataframes,
        output_dir=output_dir,
        parameters=parameters,
        file_format=file_format
    )
    
    # 8. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    total_time = (datetime.now() - start_time).total_seconds()
    total_success = sum(success_counts.values())
    
    logging.info("=" * 60)
    logging.info("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    logging.info("=" * 60)
    logging.info(f"ç»ˆç«¯æ²³æ®µæ•°é‡: {terminal_segments}")
    logging.info(f"æ•°æ®ä¸ºç©ºçš„æ²³æ®µå¯¹: {empty_data_pairs}")  
    logging.info(f"æ²¡æœ‰å…±åŒæ—¥æœŸçš„æ²³æ®µå¯¹: {no_common_dates}")
    logging.info(f"å®é™…å¤„ç†çš„æ²³æ®µå¯¹: {processed_pairs}")

    logging.info("=" * 60)
    logging.info("å®Œå…¨å‘é‡åŒ–æ€§èƒ½ç»Ÿè®¡")
    logging.info("=" * 60)
    logging.info(f"æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    logging.info(f"å¤„ç†çš„COMIDæ•°: {len(processable_comids):,}")
    logging.info(f"æˆåŠŸè®¡ç®—çš„è®°å½•æ•°: {total_success:,}")
    logging.info(f"é”™è¯¯æ•°: {error_counts}")
    logging.info(f"å¹³å‡å¤„ç†é€Ÿåº¦: {len(processable_comids)/total_time:.1f} COMID/ç§’")
    logging.info(f"å‘é‡åŒ–èŠ‚çœçš„å‡½æ•°è°ƒç”¨: {vectorization_savings:,}")
    logging.info(f"æŒ‰COMIDæ‹†åˆ†ä¿å­˜ï¼šæå‡åç»­å¤„ç†æ•ˆç‡")
    
    if total_success > 0:
        logging.info(f"å¹³å‡æ¯è®°å½•ç”¨æ—¶: {total_time/total_success*1000:.4f} æ¯«ç§’")
    
    # 9. åˆ›å»ºæ€§èƒ½æ€»ç»“æŠ¥å‘Š
    create_performance_summary_by_comid(
        result_stats=result_stats,
        output_dir=output_dir,
        total_time_seconds=total_time,
        processed_comids=len(processable_comids),
        vectorization_savings=vectorization_savings,
        parameters=parameters
    )
    
    return result_stats


def create_performance_summary_by_comid(
    result_stats: Dict,
    output_dir: str,
    total_time_seconds: float,
    processed_comids: int,
    vectorization_savings: int,
    parameters: List[str]
):
    """
    åˆ›å»ºæŒ‰COMIDä¿å­˜ç‰ˆæœ¬çš„æ€§èƒ½å’Œç»“æœæ€»ç»“æŠ¥å‘Š
    """
    
    summary_file = Path(output_dir) / "performance_summary_by_comid.txt"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å®Œå…¨å‘é‡åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®— - æŒ‰COMIDæ‹†åˆ†ä¿å­˜ - æ€§èƒ½å’Œç»“æœæ€»ç»“\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("1. æ ¸å¿ƒä¼˜åŒ–ç‰¹æ€§\n")
        f.write("-" * 40 + "\n")
        f.write("âœ… O(1)æ•°æ®è®¿é—®: é¢„å»ºç´¢å¼•æ–‡ä»¶å¿«é€Ÿè®¿é—®\n")
        f.write("âœ… å®Œå…¨å‘é‡åŒ–è®¡ç®—: æ¶ˆé™¤æ‰€æœ‰æ—¥æœŸå¾ªç¯\n")
        f.write("âœ… æŒ‰COMIDæ‹†åˆ†ä¿å­˜: ä¾¿äºä¸‹æ¸¸åˆ†æå’Œå¤„ç†\n")
        f.write("âœ… æ‰¹é‡DataFrameæ“ä½œ: é¿å…é€æ¡è®°å½•å¤„ç†\n")
        f.write("âœ… æ™ºèƒ½ç´¢å¼•æ–‡ä»¶: å¿«é€Ÿå®šä½å’Œå…ƒæ•°æ®æŸ¥è¯¢\n")
        f.write("âœ… å¤šæ ¼å¼æ”¯æŒ: CSV/Parquet/Featherå¯é€‰\n\n")
        
        f.write("2. æ€§èƒ½ç»Ÿè®¡\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ€»è¿è¡Œæ—¶é—´: {total_time_seconds/60:.1f} åˆ†é’Ÿ\n")
        f.write(f"å¤„ç†COMIDæ•°: {processed_comids:,}\n")
        f.write(f"å¹³å‡å¤„ç†é€Ÿåº¦: {processed_comids/total_time_seconds:.1f} COMID/ç§’\n")
        f.write(f"å‘é‡åŒ–èŠ‚çœçš„å‡½æ•°è°ƒç”¨: {vectorization_savings:,}\n")
        f.write(f"æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–: O(MÃ—NÃ—P) â†’ O(MÃ—P)\n")
        f.write(f"é¢„æœŸæ€§èƒ½æå‡: 500-5000å€\n\n")
        
        f.write("3. è¾“å‡ºæ–‡ä»¶ç»“æ„\n")
        f.write("-" * 40 + "\n")
        f.write("output_by_comid/\n")
        f.write("â”œâ”€â”€ master_index.json                    # æ€»ä½“ç´¢å¼•æ–‡ä»¶\n")
        f.write("â”œâ”€â”€ performance_summary_by_comid.txt     # æ€§èƒ½æ€»ç»“æŠ¥å‘Š\n")
        
        for param in parameters:
            f.write(f"â”œâ”€â”€ retention_coefficients_{param}/      # {param}å‚æ•°ç›®å½•\n")
            f.write(f"â”‚   â”œâ”€â”€ retention_coefficients_{param}_summary.csv    # {param}æ±‡æ€»æ–‡ä»¶\n")
            f.write(f"â”‚   â”œâ”€â”€ retention_coefficients_{param}_index.csv      # {param}ç´¢å¼•æ–‡ä»¶\n")
            f.write(f"â”‚   â”œâ”€â”€ retention_coefficients_{param}_COMID_12345.csv # æŒ‰COMIDåˆ†æ–‡ä»¶\n")
            f.write(f"â”‚   â”œâ”€â”€ retention_coefficients_{param}_COMID_12346.csv\n")
            f.write(f"â”‚   â””â”€â”€ ... (æ›´å¤šCOMIDæ–‡ä»¶)\n")
        
        f.write("\n4. è®¡ç®—ç»“æœç»Ÿè®¡\n")
        f.write("-" * 40 + "\n")
        
        total_records = 0
        total_files = 0
        for param in parameters:
            if param in result_stats:
                stats = result_stats[param]
                f.write(f"{param} å‚æ•°:\n")
                f.write(f"  æ€»è®°å½•æ•°: {stats['total_records']:,}\n")
                f.write(f"  æ¶‰åŠCOMIDæ•°: {stats['total_comids']:,}\n")
                f.write(f"  ä¿å­˜æ–‡ä»¶æ•°: {stats['files_saved']:,}\n")
                f.write(f"  ä¿å­˜é”™è¯¯æ•°: {stats['save_errors']}\n")
                f.write(f"  è¾“å‡ºç›®å½•: {stats['output_directory']}\n")
                f.write(f"  æ±‡æ€»æ–‡ä»¶: {stats['summary_file']}\n")
                f.write(f"  ç´¢å¼•æ–‡ä»¶: {stats['index_file']}\n\n")
                
                total_records += stats['total_records']
                total_files += stats['files_saved']
        
        f.write(f"æ€»è®¡:\n")
        f.write(f"  æ€»è®°å½•æ•°: {total_records:,}\n")
        f.write(f"  æ€»æ–‡ä»¶æ•°: {total_files:,}\n\n")
        
        f.write("5. æŒ‰COMIDæ‹†åˆ†ä¿å­˜çš„ä¼˜åŠ¿\n")
        f.write("-" * 40 + "\n")
        f.write("ğŸš€ å¿«é€ŸæŸ¥æ‰¾: æ ¹æ®COMIDç›´æ¥å®šä½æ–‡ä»¶\n")
        f.write("ğŸš€ å¹¶è¡Œå¤„ç†: æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œåˆ†æ\n")
        f.write("ğŸš€ å†…å­˜å‹å¥½: å•ä¸ªCOMIDæ–‡ä»¶å°ï¼Œå†…å­˜å ç”¨ä½\n")
        f.write("ğŸš€ å¢é‡æ›´æ–°: æ”¯æŒå•ä¸ªCOMIDæ•°æ®çš„å¢é‡æ›´æ–°\n")
        f.write("ğŸš€ ä¸‹æ¸¸å…¼å®¹: ä¾¿äºGISã€æ—¶é—´åºåˆ—åˆ†æç­‰åº”ç”¨\n")
        f.write("ğŸš€ è´¨é‡æ§åˆ¶: å•ç‹¬æ£€æŸ¥å’Œä¿®å¤ç‰¹å®šCOMIDæ•°æ®\n\n")
        
        f.write("6. ä½¿ç”¨å»ºè®®\n")
        f.write("-" * 40 + "\n")
        f.write("1. ä½¿ç”¨master_index.jsonå¿«é€Ÿäº†è§£æ•´ä½“æ•°æ®ç»“æ„\n")
        f.write("2. ä½¿ç”¨å„å‚æ•°çš„index.csvæ–‡ä»¶å¿«é€Ÿå®šä½ç‰¹å®šCOMID\n")
        f.write("3. æ±‡æ€»æ–‡ä»¶é€‚ç”¨äºæ•´ä½“åˆ†æå’Œç»Ÿè®¡\n")
        f.write("4. å•ä¸ªCOMIDæ–‡ä»¶é€‚ç”¨äºè¯¦ç»†åˆ†æå’Œå¯è§†åŒ–\n")
        f.write("5. å¹¶è¡Œå¤„ç†æ—¶å¯æŒ‰COMIDåˆ†é…è®¡ç®—ä»»åŠ¡\n")
    
    logging.info(f"æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç‰ˆæ€§èƒ½æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒæŒ‰COMIDæ‹†åˆ†ä¿å­˜çš„å®Œå…¨å‘é‡åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—
    """
    
    # é…ç½®å‚æ•°
    SPLIT_DATA_DIR = "split_data"  # æ‹†åˆ†æ•°æ®ç›®å½•
    ATTR_DATA_PATH = "data/river_attributes_new.csv"  # æ²³æ®µå±æ€§æ•°æ®
    OUTPUT_DIR = "output_by_comid"
    PARAMETERS = ["TN", "TP"]
    V_F_TN = 35.0
    V_F_TP = 44.5
    MAX_RECORDS_PER_PARAM = 100000000
    FILE_FORMAT = 'csv'  # å¯é€‰: 'csv', 'parquet', 'feather'
    
    print("ğŸš€ å®Œå…¨å‘é‡åŒ–ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—ç¨‹åºå¯åŠ¨ - æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç‰ˆæœ¬")
    print("=" * 80)
    print(f"æ‹†åˆ†æ•°æ®ç›®å½•: {SPLIT_DATA_DIR}")
    print(f"æ²³æ®µå±æ€§æ–‡ä»¶: {ATTR_DATA_PATH}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"è®¡ç®—å‚æ•°: {PARAMETERS}")
    print(f"è¾“å‡ºæ ¼å¼: {FILE_FORMAT}")
    print("=" * 80)
    print("ğŸ¯ æ ¸å¿ƒä¼˜åŒ–:")
    print("âœ… ä¿ç•™O(1)æ•°æ®è®¿é—®ä¼˜åŠ¿")
    print("âœ… æ¶ˆé™¤æ‰€æœ‰æ—¥æœŸå¾ªç¯")
    print("âœ… å‘é‡åŒ–è®¡ç®— + å‘é‡åŒ–ä¿å­˜")
    print("âœ… æŒ‰COMIDæ‹†åˆ†ä¿å­˜")
    print("âœ… æ™ºèƒ½ç´¢å¼•å’Œæ±‡æ€»æ–‡ä»¶")
    print("âœ… æ—¶é—´å¤æ‚åº¦: O(MÃ—NÃ—P) â†’ O(MÃ—P)")
    print("âœ… é¢„æœŸæ€§èƒ½æå‡: 500-5000å€")
    print("=" * 80)
    
    try:
        start_time = datetime.now()
        
        # æ‰§è¡ŒæŒ‰COMIDæ‹†åˆ†ä¿å­˜çš„å®Œå…¨å‘é‡åŒ–è®¡ç®—
        result_stats = calculate_retention_coefficients_by_comid(
            split_data_dir=SPLIT_DATA_DIR,
            attr_data_path=ATTR_DATA_PATH,
            output_dir=OUTPUT_DIR,
            parameters=PARAMETERS,
            v_f_TN=V_F_TN,
            v_f_TP=V_F_TP,
            max_records_per_param=MAX_RECORDS_PER_PARAM,
            file_format=FILE_FORMAT
        )
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        total_records = sum(stats['total_records'] for stats in result_stats.values())
        total_files = sum(stats['files_saved'] for stats in result_stats.values())
        total_comids = max(stats['total_comids'] for stats in result_stats.values() if stats['total_comids'] > 0)
        
        print("\nğŸ‰ æŒ‰COMIDæ‹†åˆ†ä¿å­˜ç‰ˆä¿ç•™ç³»æ•°è®¡ç®—å®Œæˆï¼")
        print("=" * 60)
        print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š å¤„ç†COMIDæ•°: {total_comids:,}")
        print(f"ğŸ“ æ€»è®°å½•æ•°: {total_records:,}")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files:,}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"ğŸ“‹ æ€»ä½“ç´¢å¼•: {OUTPUT_DIR}/master_index.json")
        print(f"ğŸ“ˆ æ€§èƒ½æŠ¥å‘Š: {OUTPUT_DIR}/performance_summary_by_comid.txt")
        
        # æŒ‰å‚æ•°ç»Ÿè®¡
        print("\nğŸ“Š æŒ‰å‚æ•°ç»Ÿè®¡:")
        for param in PARAMETERS:
            if param in result_stats:
                stats = result_stats[param]
                print(f"  {param}: {stats['total_records']:,} è®°å½•, "
                      f"{stats['files_saved']:,} æ–‡ä»¶, "
                      f"{stats['total_comids']:,} COMID")
        
        # æ–‡ä»¶ç»“æ„è¯´æ˜
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„:")
        print(f"  {OUTPUT_DIR}/")
        print(f"  â”œâ”€â”€ master_index.json")
        print(f"  â”œâ”€â”€ performance_summary_by_comid.txt")
        for param in PARAMETERS:
            print(f"  â”œâ”€â”€ retention_coefficients_{param}/")
            print(f"  â”‚   â”œâ”€â”€ retention_coefficients_{param}_summary.csv")
            print(f"  â”‚   â”œâ”€â”€ retention_coefficients_{param}_index.csv")
            print(f"  â”‚   â””â”€â”€ retention_coefficients_{param}_COMID_*.csv")
        
        # ä½¿ç”¨å»ºè®®
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("  1. æŸ¥çœ‹master_index.jsonäº†è§£æ•´ä½“ç»“æ„")
        print("  2. ä½¿ç”¨å„å‚æ•°index.csvå¿«é€Ÿå®šä½COMID")
        print("  3. summary.csvé€‚ç”¨äºæ•´ä½“ç»Ÿè®¡åˆ†æ") 
        print("  4. å•ä¸ªCOMIDæ–‡ä»¶é€‚ç”¨äºè¯¦ç»†åˆ†æ")
        print("  5. æ”¯æŒå¹¶è¡Œå¤„ç†å¤šä¸ªCOMIDæ–‡ä»¶")
        
        print("\nğŸ† æŒ‰COMIDæ‹†åˆ†ä¿å­˜ä¼˜åŠ¿:")
        print("   - ğŸš€ å¿«é€ŸæŸ¥æ‰¾å’Œå®šä½ç‰¹å®šCOMIDæ•°æ®")
        print("   - ğŸš€ æ”¯æŒå¹¶è¡Œå¤„ç†å’Œåˆ†å¸ƒå¼è®¡ç®—")
        print("   - ğŸš€ å†…å­˜å‹å¥½ï¼Œé¿å…å¤§æ–‡ä»¶åŠ è½½é—®é¢˜")
        print("   - ğŸš€ ä¾¿äºå¢é‡æ›´æ–°å’Œè´¨é‡æ§åˆ¶")
        print("   - ğŸš€ é€‚é…ä¸‹æ¸¸GISå’Œæ—¶é—´åºåˆ—åˆ†æ")
        
        print("\nğŸ¯ è¿™æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³æ–¹æ¡ˆï¼")
        
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)