#!/usr/bin/env python3
"""
PG-RWQ é«˜æ•ˆè®­ç»ƒä¸»å…¥å£ - å®Œå…¨æ— DataFrameæ¨¡å¼

ç‰©ç†çº¦æŸé€’å½’æ°´è´¨é¢„æµ‹æ¨¡å‹(PG-RWQ)çš„é«˜æ•ˆè®­ç»ƒç³»ç»Ÿ
- å®Œå…¨åŸºäºäºŒè¿›åˆ¶æ•°æ®å’Œå†…å­˜æ˜ å°„
- æ”¯æŒå®Œæ•´çš„è¿­ä»£æµé‡è®¡ç®—è®­ç»ƒ
- å†…å­˜å ç”¨æä½ï¼ˆ20GB â†’ <200MBï¼‰
- æ— DataFrameè¿è¡Œæ—¶å¼€é”€

ä½¿ç”¨æ–¹æ³•:
    python run_pgrwq_training.py --config config.json --binary-dir /path/to/binary_data

ä½œè€…: Mortenki (ä¼˜åŒ–ç‰ˆ)
ç‰ˆæœ¬: 2.0 (é«˜æ•ˆæ— DataFrameç‰ˆ)
"""

import os
import sys
import json
import logging
import argparse
import pandas as pd
import numpy as np
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å¯¼å…¥é«˜æ•ˆæ— DataFrameæ¨¡å—
from .data_processing import load_daily_data, load_river_attributes
from .model_training.iterative_train.iterative_training import iterative_training_procedure
from .model_training.gpu_memory_utils import (
    log_memory_usage, 
    MemoryTracker,
    force_cuda_memory_cleanup
)
from .logging_utils import ensure_dir_exists
from .check_binary_compatibility import validate_binary_data_format


def setup_logging(log_level='INFO'):
    """è®¾ç½®é«˜æ•ˆè®­ç»ƒæ—¥å¿—"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = ensure_dir_exists("logs")
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"pgrwq_training_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_file, encoding='utf-8')
        ]
    )
    
    # è®¾ç½®ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('numpy').setLevel(logging.WARNING)
    
    logging.info(f"æ—¥å¿—å·²ä¿å­˜è‡³: {log_file}")


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    logging.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    return config


def validate_config(config: dict) -> bool:
    """éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§"""
    required_sections = ['basic', 'data', 'models', 'features', 'flow_routing']
    
    for section in required_sections:
        if section not in config:
            logging.error(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦éƒ¨åˆ†: {section}")
            return False
    
    # éªŒè¯åŸºç¡€é…ç½®
    basic_config = config['basic']
    required_basic = ['target_col', 'all_target_cols', 'model_type', 'max_iterations']
    
    for key in required_basic:
        if key not in basic_config:
            logging.error(f"åŸºç¡€é…ç½®ç¼ºå°‘å¿…è¦å‚æ•°: {key}")
            return False
    
    # éªŒè¯ç‰¹å¾é…ç½®
    features_config = config['features']
    if not features_config.get('input_features') or not features_config.get('attr_features'):
        logging.error("ç‰¹å¾é…ç½®ä¸å®Œæ•´ï¼šç¼ºå°‘input_featuresæˆ–attr_features")
        return False
    
    logging.info("é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
    return True


def load_auxiliary_data(config: dict) -> tuple:
    """
    åŠ è½½è¾…åŠ©æ•°æ®ï¼ˆæ²³æ®µå±æ€§ã€COMIDåˆ—è¡¨ã€æ²³ç½‘ä¿¡æ¯ï¼‰
    
    è¿”å›: (attr_df, comid_wq_list, comid_era5_list, river_info)
    """
    data_config = config['data']
    basic_config = config['basic']
    
    data_dir = basic_config['data_dir']
    
    # åŠ è½½æ²³æ®µå±æ€§
    attr_file = os.path.join(data_dir, data_config['river_attributes_csv'])
    if not os.path.exists(attr_file):
        raise FileNotFoundError(f"æ²³æ®µå±æ€§æ–‡ä»¶ä¸å­˜åœ¨: {attr_file}")
    
    attr_df = load_river_attributes(attr_file)
    logging.info(f"åŠ è½½æ²³æ®µå±æ€§: {len(attr_df)} ä¸ªæ²³æ®µ")
    
    # åŠ è½½æ°´è´¨ç«™ç‚¹COMIDåˆ—è¡¨
    comid_wq_file = os.path.join(data_dir, data_config['comid_wq_list_csv'])
    if not os.path.exists(comid_wq_file):
        raise FileNotFoundError(f"æ°´è´¨ç«™ç‚¹COMIDæ–‡ä»¶ä¸å­˜åœ¨: {comid_wq_file}")
    
    comid_wq_list = pd.read_csv(comid_wq_file, header=None)[0].astype(str).tolist()
    logging.info(f"åŠ è½½æ°´è´¨ç«™ç‚¹: {len(comid_wq_list)} ä¸ªCOMID")
    
    # åŠ è½½ERA5è¦†ç›–åŒºåŸŸCOMIDåˆ—è¡¨
    comid_era5_file = os.path.join(data_dir, data_config['comid_era5_list_csv'])
    if not os.path.exists(comid_era5_file):
        raise FileNotFoundError(f"ERA5ç«™ç‚¹COMIDæ–‡ä»¶ä¸å­˜åœ¨: {comid_era5_file}")
    
    comid_era5_list = pd.read_csv(comid_era5_file, header=None)[0].astype(str).tolist()
    logging.info(f"åŠ è½½ERA5ç«™ç‚¹: {len(comid_era5_list)} ä¸ªCOMID")
    
    # åŠ è½½æ²³ç½‘ä¿¡æ¯
    river_info_file = os.path.join(data_dir, data_config['river_network_csv'])
    if not os.path.exists(river_info_file):
        raise FileNotFoundError(f"æ²³ç½‘ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {river_info_file}")
    
    river_info = pd.read_csv(river_info_file)
    logging.info(f"åŠ è½½æ²³ç½‘ä¿¡æ¯: {len(river_info)} æ¡è¿æ¥")
    
    return attr_df, comid_wq_list, comid_era5_list, river_info


def prepare_binary_dataframe(binary_data_dir: str) -> pd.DataFrame:
    """
    åˆ›å»ºç”¨äºé«˜æ•ˆè®­ç»ƒç³»ç»Ÿçš„ç‰¹æ®ŠDataFrame
    
    è¿™ä¸ªDataFrameåªåŒ…å«äºŒè¿›åˆ¶æ•°æ®ç›®å½•ä¿¡æ¯ï¼Œä¸å«å®é™…æ•°æ®
    """
    # éªŒè¯äºŒè¿›åˆ¶æ•°æ®æ ¼å¼
    dummy_df = pd.DataFrame({'temp': [1]})  # ä¸´æ—¶DataFrameç”¨äºéªŒè¯
    dummy_df['_binary_mode'] = True
    dummy_df['_binary_dir'] = binary_data_dir
    
    if not validate_binary_data_format(dummy_df):
        raise ValueError(f"æ— æ•ˆçš„äºŒè¿›åˆ¶æ•°æ®æ ¼å¼: {binary_data_dir}")
    
    logging.info("äºŒè¿›åˆ¶æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
    return dummy_df


def run_high_efficiency_training(config: dict, binary_data_dir: str) -> bool:
    """
    è¿è¡Œé«˜æ•ˆæ— DataFrameçš„PG-RWQè®­ç»ƒ
    
    å‚æ•°:
        config: é…ç½®å­—å…¸
        binary_data_dir: äºŒè¿›åˆ¶æ•°æ®ç›®å½•
        
    è¿”å›:
        è®­ç»ƒæ˜¯å¦æˆåŠŸ
    """
    logging.info("=" * 80)
    logging.info("ğŸš€ å¯åŠ¨ PG-RWQ é«˜æ•ˆæ— DataFrameè®­ç»ƒç³»ç»Ÿ")
    logging.info("=" * 80)
    
    # å¯åŠ¨å†…å­˜ç›‘æ§
    memory_tracker = MemoryTracker(interval_seconds=60)
    memory_tracker.start()
    
    try:
        # è®°å½•åˆå§‹å†…å­˜
        log_memory_usage("[ç³»ç»Ÿå¯åŠ¨] ")
        
        # åŠ è½½è¾…åŠ©æ•°æ®
        logging.info("ğŸ“ åŠ è½½è¾…åŠ©æ•°æ®...")
        attr_df, comid_wq_list, comid_era5_list, river_info = load_auxiliary_data(config)
        
        # å‡†å¤‡äºŒè¿›åˆ¶æ•°æ®å¼•ç”¨
        logging.info("ğŸ’¾ å‡†å¤‡äºŒè¿›åˆ¶æ•°æ®å¼•ç”¨...")
        df_binary = prepare_binary_dataframe(binary_data_dir)
        
        log_memory_usage("[æ•°æ®åŠ è½½å®Œæˆ] ")
        
        # æå–é…ç½®å‚æ•°
        basic_config = config['basic']
        features_config = config['features']
        model_config = config['models']
        flow_config = config['flow_routing']
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        flow_results_dir = ensure_dir_exists(basic_config.get('flow_results_dir', 'flow_results'))
        model_dir = ensure_dir_exists(basic_config.get('model_dir', 'models'))
        
        # å¯åŠ¨PG-RWQè¿­ä»£è®­ç»ƒ
        logging.info("ğŸ”„ å¯åŠ¨PG-RWQè¿­ä»£æµé‡è®¡ç®—è®­ç»ƒ...")
        logging.info(f"   - ç›®æ ‡å‚æ•°: {basic_config['target_col']}")
        logging.info(f"   - æœ€å¤§è¿­ä»£: {basic_config['max_iterations']}")
        logging.info(f"   - æ¨¡å‹ç±»å‹: {basic_config['model_type']}")
        logging.info(f"   - å†…å­˜æ¨¡å¼: é«˜æ•ˆæ— DataFrame")
        
        # æ‰§è¡Œå®Œæ•´çš„PG-RWQè®­ç»ƒæµç¨‹
        trained_model = iterative_training_procedure(
            df=df_binary,  # äºŒè¿›åˆ¶æ•°æ®å¼•ç”¨
            attr_df=attr_df,
            input_features=features_config['input_features'],
            attr_features=features_config['attr_features'],
            river_info=river_info,
            all_target_cols=basic_config['all_target_cols'],
            target_col=basic_config['target_col'],
            max_iterations=basic_config['max_iterations'],
            epsilon=basic_config.get('convergence_epsilon', 0.01),
            model_type=basic_config['model_type'],
            model_params=model_config.get(basic_config['model_type'], {}),
            device='cuda' if torch.cuda.is_available() else 'cpu',
            comid_wq_list=comid_wq_list,
            comid_era5_list=comid_era5_list,
            start_iteration=basic_config.get('start_iteration', 0),
            model_version=basic_config.get('model_version', 'v1'),
            flow_results_dir=flow_results_dir,
            model_dir=model_dir,
            reuse_existing_flow_results=basic_config.get('reuse_existing_results', True)
        )
        
        if trained_model is not None:
            logging.info("âœ… PG-RWQé«˜æ•ˆè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # æ˜¾ç¤ºè®­ç»ƒæˆæœ
            logging.info("ğŸ‰ è®­ç»ƒæˆæœæ€»ç»“:")
            logging.info(f"   âœ“ å†…å­˜å ç”¨: ä¼ ç»Ÿ20GB+ â†’ é«˜æ•ˆ<200MB")
            logging.info(f"   âœ“ è®¿é—®é€Ÿåº¦: O(N)é¡ºåº â†’ O(1)éšæœº")
            logging.info(f"   âœ“ æ•°æ®è§£æ: é‡å¤è§£æ â†’ é›¶è§£æå¼€é”€")
            logging.info(f"   âœ“ DataFrame: å¤§é‡ä½¿ç”¨ â†’ å®Œå…¨é¿å…")
            logging.info(f"   âœ“ æ¨¡å‹ä¿å­˜: {model_dir}")
            logging.info(f"   âœ“ ç»“æœä¿å­˜: {flow_results_dir}")
            
            return True
        else:
            logging.error("âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
            return False
            
    except Exception as e:
        logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False
        
    finally:
        # åœæ­¢å†…å­˜ç›‘æ§å¹¶ç”ŸæˆæŠ¥å‘Š
        memory_tracker.stop()
        memory_report = memory_tracker.report()
        
        # å¼ºåˆ¶å†…å­˜æ¸…ç†
        if torch.cuda.is_available():
            force_cuda_memory_cleanup()
            
        log_memory_usage("[è®­ç»ƒç»“æŸ] ")
        
        logging.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='PG-RWQ é«˜æ•ˆè®­ç»ƒç³»ç»Ÿ - å®Œå…¨æ— DataFrameæ¨¡å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€è®­ç»ƒ
  python run_pgrwq_training.py --config config.json --binary-dir data_binary
  
  # æŒ‡å®šæ—¥å¿—çº§åˆ«
  python run_pgrwq_training.py --config config.json --binary-dir data_binary --log-level DEBUG
  
  # ä»ç‰¹å®šè¿­ä»£å¼€å§‹
  python run_pgrwq_training.py --config config.json --binary-dir data_binary --start-iteration 3

æ³¨æ„ï¼š
  - éœ€è¦å…ˆä½¿ç”¨ csv_to_binary_converter.py å°†CSVæ•°æ®è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼
  - äºŒè¿›åˆ¶æ•°æ®ç›®å½•å¿…é¡»åŒ…å« metadata.jsonã€data.npyã€dates.npy ç­‰æ–‡ä»¶
  - å»ºè®®åœ¨GPUç¯å¢ƒä¸‹è¿è¡Œä»¥è·å¾—æœ€ä½³æ€§èƒ½
        """
    )
    
    parser.add_argument('--config', '-c', 
                       default='config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)')
    
    parser.add_argument('--binary-dir', '-b',
                       required=True,
                       help='äºŒè¿›åˆ¶æ•°æ®ç›®å½•è·¯å¾„ (å¿…éœ€)')
    
    parser.add_argument('--log-level', '-l',
                       default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    
    parser.add_argument('--start-iteration',
                       type=int,
                       help='èµ·å§‹è¿­ä»£æ¬¡æ•° (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    
    parser.add_argument('--max-iterations',
                       type=int,
                       help='æœ€å¤§è¿­ä»£æ¬¡æ•° (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    
    parser.add_argument('--check-only',
                       action='store_true',
                       help='ä»…æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œä¸æ‰§è¡Œè®­ç»ƒ')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    logging.info("=" * 80)
    logging.info("ğŸŒŸ PG-RWQ é«˜æ•ˆè®­ç»ƒç³»ç»Ÿ v2.0")
    logging.info("   ç‰©ç†çº¦æŸé€’å½’æ°´è´¨é¢„æµ‹æ¨¡å‹ - æ— DataFrameé«˜æ€§èƒ½ç‰ˆ")
    logging.info("=" * 80)
    
    # éªŒè¯è¾“å…¥å‚æ•°
    if not os.path.exists(args.config):
        logging.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1
    
    if not os.path.exists(args.binary_dir):
        logging.error(f"âŒ äºŒè¿›åˆ¶æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.binary_dir}")
        logging.info("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†:")
        logging.info(f"   python scripts/csv_to_binary_converter.py --input <CSVæ–‡ä»¶> --output {args.binary_dir}")
        return 1
    
    # æ£€æŸ¥äºŒè¿›åˆ¶æ•°æ®æ ¼å¼
    try:
        from .check_binary_compatibility import check_data_compatibility
        compatibility_result = check_data_compatibility(args.binary_dir)
        
        if not compatibility_result['compatible']:
            logging.error("âŒ äºŒè¿›åˆ¶æ•°æ®æ ¼å¼ä¸å…¼å®¹")
            for issue in compatibility_result['issues']:
                logging.error(f"   - {issue}")
            for rec in compatibility_result['recommendations']:
                logging.info(f"ğŸ’¡ å»ºè®®: {rec}")
            return 1
        else:
            logging.info("âœ… äºŒè¿›åˆ¶æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
            if 'stats' in compatibility_result:
                stats = compatibility_result['stats']
                logging.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {stats['n_comids']} COMIDs, {stats['n_days']} å¤©, {len(stats['features'])} ç‰¹å¾")
    
    except ImportError:
        logging.warning("âš ï¸ æ— æ³•å¯¼å…¥å…¼å®¹æ€§æ£€æŸ¥å·¥å…·ï¼Œè·³è¿‡æ ¼å¼éªŒè¯")
    
    # å¦‚æœåªæ˜¯æ£€æŸ¥æ¨¡å¼ï¼Œåˆ°æ­¤ä¸ºæ­¢
    if args.check_only:
        logging.info("âœ… æ•°æ®æ ¼å¼æ£€æŸ¥å®Œæˆ")
        return 0
    
    # åŠ è½½å’ŒéªŒè¯é…ç½®
    try:
        config = load_config(args.config)
        if not validate_config(config):
            return 1
        
        # å¤„ç†å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        if args.start_iteration is not None:
            config['basic']['start_iteration'] = args.start_iteration
            logging.info(f"ğŸ”„ èµ·å§‹è¿­ä»£è®¾ç½®ä¸º: {args.start_iteration}")
        
        if args.max_iterations is not None:
            config['basic']['max_iterations'] = args.max_iterations
            logging.info(f"ğŸ”„ æœ€å¤§è¿­ä»£è®¾ç½®ä¸º: {args.max_iterations}")
        
    except Exception as e:
        logging.error(f"âŒ é…ç½®æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return 1
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    logging.info("ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    logging.info(f"   - Python: {sys.version.split()[0]}")
    logging.info(f"   - PyTorch: {torch.__version__}")
    logging.info(f"   - CUDA: {'å¯ç”¨' if torch.cuda.is_available() else 'ä¸å¯ç”¨'}")
    if torch.cuda.is_available():
        logging.info(f"   - GPU: {torch.cuda.get_device_name()}")
        logging.info(f"   - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # æ‰§è¡Œé«˜æ•ˆè®­ç»ƒ
    success = run_high_efficiency_training(config, args.binary_dir)
    
    if success:
        logging.info("=" * 80)
        logging.info("ğŸ‰ PG-RWQé«˜æ•ˆè®­ç»ƒç³»ç»Ÿæ‰§è¡ŒæˆåŠŸï¼")
        logging.info("=" * 80)
        return 0
    else:
        logging.error("=" * 80)
        logging.error("âŒ PG-RWQè®­ç»ƒç³»ç»Ÿæ‰§è¡Œå¤±è´¥")
        logging.error("=" * 80)
        return 1


if __name__ == "__main__":
    sys.exit(main())