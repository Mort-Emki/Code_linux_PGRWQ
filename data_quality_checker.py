"""
data_quality_checker.py - ç»Ÿä¸€çš„æ•°æ®è´¨é‡æ£€æŸ¥æ¨¡å—

æä¾›æ ‡å‡†åŒ–çš„æ•°æ®å¼‚å¸¸æ£€æµ‹å’Œä¿®å¤åŠŸèƒ½ï¼Œæ¶ˆé™¤é¡¹ç›®ä¸­çš„é‡å¤ä»£ç ã€‚

âš ï¸ é‡è¦è¯´æ˜ï¼šæŠ½æ ·æ£€æµ‹ vs å®Œæ•´æ£€æµ‹
=======================================

1. å®Œæ•´æ•°æ®æ£€æµ‹ (comprehensive_data_quality_check)
   - æ£€æµ‹å®Œæ•´æ•°æ®é›†ï¼Œå¯é€‰æ‹©ä¿®å¤å¼‚å¸¸
   - ç”¨äºï¼šregression_main.py, main.py ç­‰å®Œæ•´æ•°æ®å¤„ç†åœºæ™¯
   - ä¿®å¤èŒƒå›´ï¼šæ•´ä¸ªæ•°æ®é›†
   
2. æŠ½æ ·æ•°æ®æ£€æµ‹ (sample_based_data_quality_check)  
   - ä»…æ£€æµ‹æŠ½æ ·æ•°æ®è´¨é‡ï¼Œä¸ä¿®å¤æŠ½æ ·æ•°æ®ï¼ˆé¿å…æ•°æ®ä¸ä¸€è‡´ï¼‰
   - ç”¨äºï¼šrun_efficient_training.py ç­‰é«˜æ•ˆè®­ç»ƒåœºæ™¯
   - æ—¶é—´åºåˆ—ï¼šä»…æ£€æµ‹ï¼ˆæ ·æœ¬ä¿®å¤æ— æ„ä¹‰ï¼‰
   - å±æ€§æ•°æ®ï¼šå¯ä¿®å¤ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
   - ç›®çš„ï¼šå¿«é€Ÿè¯„ä¼°æ•°æ®è´¨é‡ï¼Œä¸ºåç»­å¤„ç†æä¾›å‚è€ƒ

3. æ‰¹å¤„ç†æ•°æ®æ£€æµ‹ (ç”¨äºCSVè½¬æ¢)
   - æ£€æµ‹æ¯ä¸ªæ•°æ®å—ï¼Œå¯é€‰æ‹©ä¿®å¤
   - ç”¨äºï¼šscripts/csv_to_binary_converter.py
   - ä¿®å¤èŒƒå›´ï¼šå½“å‰å¤„ç†çš„æ•°æ®å—
"""

import logging
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
from .data_processing import detect_and_handle_anomalies


class DataQualityConfig:
    """æ•°æ®è´¨é‡æ£€æŸ¥é…ç½®"""
    
    # æ ‡å‡†å‚æ•°é…ç½®
    QOUT_CHECK = {
        'columns_to_check': ['Qout'],
        'negative_replacement': 0.001,
        'nan_replacement': 0.001,
        'outlier_method': 'iqr',
        'outlier_threshold': 4.0,  # ç»Ÿä¸€é˜ˆå€¼
        'data_type': 'timeseries'
    }
    
    INPUT_FEATURES_CHECK = {
        'check_negative': False,  # è¾“å…¥ç‰¹å¾å¯èƒ½ä¸ºè´Ÿ
        'negative_replacement': 0.0,
        'nan_replacement': 0.0,
        'outlier_method': 'iqr',
        'outlier_threshold': 6.0,
        'data_type': 'timeseries'
    }
    
    TARGET_DATA_CHECK = {
        'check_nan': False,
        'fix_nan': False,  # æ°´è´¨æ•°æ®ä¸å¡«å……NaN
        'negative_replacement': 0.001,
        'outlier_method': 'iqr',
        'outlier_threshold': 6.0,
        'data_type': 'timeseries'
    }
    
    ATTR_DATA_CHECK = {
        'check_negative': False,
        'negative_replacement': 0.001,
        'nan_replacement': 0.001,
        'outlier_method': 'iqr',
        'outlier_threshold': 4.0,
        'data_type': 'attributes'
    }


class DataQualityChecker:
    """ç»Ÿä¸€çš„æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self, fix_anomalies: bool = False, verbose: bool = True, 
                 logger: Optional[Any] = None):
        """
        åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å™¨
        
        Args:
            fix_anomalies: æ˜¯å¦ä¿®å¤æ£€æµ‹åˆ°çš„å¼‚å¸¸
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.fix_anomalies = fix_anomalies
        self.verbose = verbose
        self.logger = logger or logging
        
    def check_qout_data(self, df: pd.DataFrame, 
                       exclude_comids: Optional[List] = None,
                       data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
        """
        æ£€æŸ¥æµé‡æ•°æ®(Qout)
        
        Args:
            df: æ•°æ®æ¡†
            exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            
        Returns:
            (å¤„ç†åçš„æ•°æ®æ¡†, æ£€æŸ¥ç»“æœ)
        """
        config = DataQualityConfig.QOUT_CHECK.copy()
        config.update({
            'fix_negative': self.fix_anomalies,
            'fix_outliers': self.fix_anomalies,
            'fix_nan': self.fix_anomalies,
            'verbose': self.verbose,
            'logger': self.logger,
            'exclude_comids': exclude_comids,
            'data_type': data_type
        })
        
        return detect_and_handle_anomalies(df, **config)
    
    def check_input_features(self, df: pd.DataFrame, 
                           input_features: List[str],
                           exclude_comids: Optional[List] = None,
                           data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
        """
        æ£€æŸ¥è¾“å…¥ç‰¹å¾æ•°æ®
        
        Args:
            df: æ•°æ®æ¡†
            input_features: è¾“å…¥ç‰¹å¾åˆ—ååˆ—è¡¨
            exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            
        Returns:
            (å¤„ç†åçš„æ•°æ®æ¡†, æ£€æŸ¥ç»“æœ)
        """
        # æ£€æŸ¥å¯ç”¨ç‰¹å¾
        available_features = [col for col in input_features if col in df.columns]
        if not available_features:
            self.logger.warning("æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„è¾“å…¥ç‰¹å¾åˆ—")
            return df, {'has_anomalies': False}
        
        config = DataQualityConfig.INPUT_FEATURES_CHECK.copy()
        config.update({
            'columns_to_check': available_features,
            'fix_nan': self.fix_anomalies,
            'verbose': self.verbose,
            'logger': self.logger,
            'exclude_comids': exclude_comids,
            'data_type': data_type
        })
        
        return detect_and_handle_anomalies(df, **config)
    
    def check_target_data(self, df: pd.DataFrame, 
                         target_cols: List[str],
                         exclude_comids: Optional[List] = None,
                         data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
        """
        æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ®
        
        Args:
            df: æ•°æ®æ¡†
            target_cols: ç›®æ ‡æ•°æ®åˆ—ååˆ—è¡¨
            exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            
        Returns:
            (å¤„ç†åçš„æ•°æ®æ¡†, æ£€æŸ¥ç»“æœ)
        """
        # æ£€æŸ¥å¯ç”¨ç›®æ ‡åˆ—
        available_cols = [col for col in target_cols if col in df.columns]
        if not available_cols:
            self.logger.warning("æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„æ°´è´¨ç›®æ ‡åˆ—")
            return df, {'has_anomalies': False}
        
        config = DataQualityConfig.TARGET_DATA_CHECK.copy()
        config.update({
            'columns_to_check': available_cols,
            'verbose': self.verbose,
            'logger': self.logger,
            'exclude_comids': exclude_comids,
            'data_type': data_type
        })
        
        return detect_and_handle_anomalies(df, **config)
    
    def check_attr_data(self, attr_df: pd.DataFrame, 
                       attr_features: List[str],
                       exclude_comids: Optional[List] = None) -> Tuple[pd.DataFrame, Dict]:
        """
        æ£€æŸ¥æ²³æ®µå±æ€§æ•°æ®
        
        Args:
            attr_df: å±æ€§æ•°æ®æ¡†
            attr_features: å±æ€§ç‰¹å¾åˆ—ååˆ—è¡¨
            exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
            
        Returns:
            (å¤„ç†åçš„æ•°æ®æ¡†, æ£€æŸ¥ç»“æœ)
        """
        # æ£€æŸ¥å¯ç”¨å±æ€§ç‰¹å¾
        available_features = [col for col in attr_features if col in attr_df.columns]
        if not available_features:
            self.logger.warning("æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„å±æ€§ç‰¹å¾åˆ—")
            return attr_df, {'has_anomalies': False}
        
        config = DataQualityConfig.ATTR_DATA_CHECK.copy()
        config.update({
            'columns_to_check': available_features,
            'fix_nan': self.fix_anomalies,
            'verbose': self.verbose,
            'logger': self.logger,
            'exclude_comids': exclude_comids
        })
        
        return detect_and_handle_anomalies(attr_df, **config)
    
    def comprehensive_check(self, df: pd.DataFrame, 
                          attr_df: pd.DataFrame,
                          input_features: List[str],
                          target_cols: List[str], 
                          attr_features: List[str],
                          exclude_comids: Optional[List] = None,
                          data_type: str = 'timeseries') -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
        """
        ç»¼åˆæ•°æ®è´¨é‡æ£€æŸ¥ï¼Œæ›¿ä»£æ‰€æœ‰é‡å¤è°ƒç”¨
        
        Args:
            df: æ—¶é—´åºåˆ—æ•°æ®æ¡†
            attr_df: å±æ€§æ•°æ®æ¡†
            input_features: è¾“å…¥ç‰¹å¾åˆ—è¡¨
            target_cols: ç›®æ ‡æ•°æ®åˆ—è¡¨
            attr_features: å±æ€§ç‰¹å¾åˆ—è¡¨
            exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            
        Returns:
            (å¤„ç†åçš„æ—¶é—´åºåˆ—æ•°æ®æ¡†, å¤„ç†åçš„å±æ€§æ•°æ®æ¡†, ç»¼åˆæ£€æŸ¥æŠ¥å‘Š)
        """
        quality_report = {}
        
        self.logger.info("å¼€å§‹ç»¼åˆæ•°æ®è´¨é‡æ£€æŸ¥...")
        
        # 1. æ£€æŸ¥æµé‡æ•°æ®
        self.logger.info("1. æ£€æŸ¥æµé‡æ•°æ®(Qout)...")
        df, qout_results = self.check_qout_data(df, exclude_comids, data_type)
        quality_report['qout'] = qout_results
        
        # 2. æ£€æŸ¥è¾“å…¥ç‰¹å¾
        if input_features:
            self.logger.info("2. æ£€æŸ¥è¾“å…¥ç‰¹å¾æ•°æ®...")
            df, input_results = self.check_input_features(df, input_features, exclude_comids, data_type)
            quality_report['input_features'] = input_results
        
        # 3. æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ®
        if target_cols:
            self.logger.info("3. æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ®...")
            df, target_results = self.check_target_data(df, target_cols, exclude_comids, data_type)
            quality_report['target_data'] = target_results
        
        # 4. æ£€æŸ¥å±æ€§æ•°æ®
        if attr_features:
            self.logger.info("4. æ£€æŸ¥æ²³æ®µå±æ€§æ•°æ®...")
            attr_df, attr_results = self.check_attr_data(attr_df, attr_features, exclude_comids)
            quality_report['attr_data'] = attr_results
        
        self.logger.info("æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        return df, attr_df, quality_report


# ä¾¿åˆ©å‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹
def check_qout_data(df: pd.DataFrame, fix_anomalies: bool = False, 
                   verbose: bool = True, logger: Optional[Any] = None,
                   exclude_comids: Optional[List] = None,
                   data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæ£€æŸ¥æµé‡æ•°æ®"""
    checker = DataQualityChecker(fix_anomalies, verbose, logger)
    return checker.check_qout_data(df, exclude_comids, data_type)


def check_input_features(df: pd.DataFrame, input_features: List[str],
                        fix_anomalies: bool = False, verbose: bool = True,
                        logger: Optional[Any] = None,
                        exclude_comids: Optional[List] = None,
                        data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæ£€æŸ¥è¾“å…¥ç‰¹å¾"""
    checker = DataQualityChecker(fix_anomalies, verbose, logger)
    return checker.check_input_features(df, input_features, exclude_comids, data_type)


def check_target_data(df: pd.DataFrame, target_cols: List[str],
                     fix_anomalies: bool = False, verbose: bool = True,
                     logger: Optional[Any] = None,
                     exclude_comids: Optional[List] = None,
                     data_type: str = 'timeseries') -> Tuple[pd.DataFrame, Dict]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæ£€æŸ¥ç›®æ ‡æ•°æ®"""
    checker = DataQualityChecker(fix_anomalies, verbose, logger)
    return checker.check_target_data(df, target_cols, exclude_comids, data_type)


def check_attr_data(attr_df: pd.DataFrame, attr_features: List[str],
                   fix_anomalies: bool = False, verbose: bool = True,
                   logger: Optional[Any] = None,
                   exclude_comids: Optional[List] = None) -> Tuple[pd.DataFrame, Dict]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæ£€æŸ¥å±æ€§æ•°æ®"""
    checker = DataQualityChecker(fix_anomalies, verbose, logger)
    return checker.check_attr_data(attr_df, attr_features, exclude_comids)


def comprehensive_data_quality_check(df: pd.DataFrame, 
                                   attr_df: pd.DataFrame,
                                   input_features: List[str],
                                   target_cols: List[str], 
                                   attr_features: List[str],
                                   fix_anomalies: bool = False,
                                   verbose: bool = True,
                                   logger: Optional[Any] = None,
                                   exclude_comids: Optional[List] = None,
                                   data_type: str = 'timeseries') -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
    """ä¾¿åˆ©å‡½æ•°ï¼šç»¼åˆæ•°æ®è´¨é‡æ£€æŸ¥"""
    checker = DataQualityChecker(fix_anomalies, verbose, logger)
    return checker.comprehensive_check(df, attr_df, input_features, target_cols, 
                                     attr_features, exclude_comids, data_type)


def sample_based_data_quality_check(sample_df: Optional[pd.DataFrame],
                                   attr_df: pd.DataFrame,
                                   input_features: List[str],
                                   target_cols: List[str], 
                                   attr_features: List[str],
                                   fix_anomalies: bool = False,
                                   verbose: bool = True,
                                   logger: Optional[Any] = None,
                                   exclude_comids: Optional[List] = None) -> Tuple[Optional[pd.DataFrame], pd.DataFrame, Dict]:
    """
    åŸºäºæŠ½æ ·æ•°æ®çš„è´¨é‡æ£€æŸ¥ï¼ˆç”¨äºé«˜æ•ˆè®­ç»ƒåœºæ™¯ï¼‰
    
    âš ï¸  é‡è¦è¯´æ˜ï¼š
    - æ—¶é—´åºåˆ—æ•°æ®ï¼šä»…æ£€æµ‹ä¸ä¿®å¤ï¼ˆå› ä¸ºåªæ˜¯æ ·æœ¬ï¼Œä¿®å¤æ ·æœ¬æ²¡æœ‰æ„ä¹‰ï¼‰
    - å±æ€§æ•°æ®ï¼šå¯ä»¥ä¿®å¤ï¼ˆå› ä¸ºæ˜¯å®Œæ•´æ•°æ®é›†ï¼‰
    
    Args:
        sample_df: æŠ½æ ·çš„æ—¶é—´åºåˆ—æ•°æ®ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        attr_df: å®Œæ•´çš„å±æ€§æ•°æ®
        input_features: è¾“å…¥ç‰¹å¾åˆ—è¡¨
        target_cols: ç›®æ ‡æ•°æ®åˆ—è¡¨
        attr_features: å±æ€§ç‰¹å¾åˆ—è¡¨
        fix_anomalies: æ˜¯å¦ä¿®å¤å¼‚å¸¸ï¼ˆä»…å¯¹å±æ€§æ•°æ®æœ‰æ•ˆï¼‰
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        logger: æ—¥å¿—è®°å½•å™¨
        exclude_comids: è¦æ’é™¤çš„COMIDåˆ—è¡¨
        
    Returns:
        (åŸå§‹æŠ½æ ·æ•°æ®, å¤„ç†åçš„å±æ€§æ•°æ®, æ£€æŸ¥æŠ¥å‘Š)
    """
    # å¯¹äºæŠ½æ ·æ•°æ®ï¼Œå¼ºåˆ¶è®¾ä¸ºä»…æ£€æµ‹æ¨¡å¼
    sample_checker = DataQualityChecker(fix_anomalies=False, verbose=verbose, logger=logger)
    # å¯¹äºå±æ€§æ•°æ®ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ä¿®å¤æ¨¡å¼  
    attr_checker = DataQualityChecker(fix_anomalies=fix_anomalies, verbose=verbose, logger=logger)
    
    quality_report = {}
    
    if logger:
        logger.info("å¼€å§‹åŸºäºæŠ½æ ·çš„æ•°æ®è´¨é‡æ£€æŸ¥...")
        logger.info("æ³¨æ„ï¼šæ—¶é—´åºåˆ—æ•°æ®ä»…æ£€æµ‹ä¸ä¿®å¤ï¼ˆæŠ½æ ·æ•°æ®ï¼‰ï¼Œå±æ€§æ•°æ®å¯é€‰æ‹©ä¿®å¤ï¼ˆå®Œæ•´æ•°æ®ï¼‰")
    
    # åˆå§‹åŒ–æ£€æŸ¥ç»“æœ
    quality_report = {
        'qout': {'has_anomalies': False, 'sample_based': True},
        'input_features': {'has_anomalies': False, 'sample_based': True}, 
        'target_data': {'has_anomalies': False, 'sample_based': True},
        'attr_data': {'has_anomalies': False, 'sample_based': False}
    }
    
    if sample_df is not None:
        # 1. æ£€æŸ¥æµé‡æ•°æ®ï¼ˆä»…æ£€æµ‹ï¼‰
        if 'Qout' in sample_df.columns:
            if logger:
                logger.info("1. æ£€æŸ¥æµé‡æ•°æ® (Qout) - åŸºäºæŠ½æ ·ï¼ˆä»…æ£€æµ‹ï¼‰...")
            try:
                _, qout_results = sample_checker.check_qout_data(
                    sample_df.copy(), exclude_comids, 'timeseries_sample'
                )
                qout_results['sample_based'] = True
                qout_results['sample_size'] = len(sample_df)
                quality_report['qout'] = qout_results
                
                if qout_results.get('has_anomalies', False):
                    if logger:
                        logger.warning("âš ï¸  æŠ½æ ·æ•°æ®ä¸­å‘ç°å¼‚å¸¸ï¼Œä½†ä¸ä¿®å¤æŠ½æ ·æ•°æ®")
                        logger.info("ğŸ’¡ å¦‚éœ€ä¿®å¤ï¼Œè¯·å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œå¤„ç†")
            except Exception as e:
                if logger:
                    logger.warning(f"æµé‡æ•°æ®æ£€æŸ¥å‡ºé”™: {e}")
        else:
            if logger:
                logger.info("1. è·³è¿‡æµé‡æ•°æ®æ£€æŸ¥ (æ•°æ®ä¸å¯ç”¨)")
        
        # 2. æ£€æŸ¥è¾“å…¥ç‰¹å¾ï¼ˆä»…æ£€æµ‹ï¼‰
        if input_features:
            if logger:
                logger.info("2. æ£€æŸ¥æ—¥å°ºåº¦è¾“å…¥ç‰¹å¾ - åŸºäºæŠ½æ ·ï¼ˆä»…æ£€æµ‹ï¼‰...")
            try:
                _, input_results = sample_checker.check_input_features(
                    sample_df.copy(), input_features, exclude_comids, 'timeseries_sample'
                )
                input_results['sample_based'] = True
                input_results['sample_size'] = len(sample_df)
                quality_report['input_features'] = input_results
                
                if input_results.get('has_anomalies', False):
                    if logger:
                        logger.warning("âš ï¸  è¾“å…¥ç‰¹å¾æŠ½æ ·æ•°æ®ä¸­å‘ç°å¼‚å¸¸ï¼Œä½†ä¸ä¿®å¤æŠ½æ ·æ•°æ®")
                        logger.info("ğŸ’¡ å¦‚éœ€ä¿®å¤ï¼Œè¯·å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œå¤„ç†")
            except Exception as e:
                if logger:
                    logger.warning(f"è¾“å…¥ç‰¹å¾æ£€æŸ¥å‡ºé”™: {e}")
        else:
            if logger:
                logger.info("2. è·³è¿‡è¾“å…¥ç‰¹å¾æ£€æŸ¥ (æ•°æ®ä¸å¯ç”¨)")
        
        # 3. æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ®ï¼ˆä»…æ£€æµ‹ï¼‰
        if target_cols:
            if logger:
                logger.info("3. æ£€æŸ¥æ°´è´¨ç›®æ ‡æ•°æ® - åŸºäºæŠ½æ ·ï¼ˆä»…æ£€æµ‹ï¼‰...")
            try:
                _, target_results = sample_checker.check_target_data(
                    sample_df.copy(), target_cols, exclude_comids, 'timeseries_sample'
                )
                target_results['sample_based'] = True
                target_results['sample_size'] = len(sample_df)
                quality_report['target_data'] = target_results
                
                if target_results.get('has_anomalies', False):
                    if logger:
                        logger.warning("âš ï¸  æ°´è´¨ç›®æ ‡æŠ½æ ·æ•°æ®ä¸­å‘ç°å¼‚å¸¸ï¼Œä½†ä¸ä¿®å¤æŠ½æ ·æ•°æ®")
                        logger.info("ğŸ’¡ å¦‚éœ€ä¿®å¤ï¼Œè¯·å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œå¤„ç†")
            except Exception as e:
                if logger:
                    logger.warning(f"æ°´è´¨ç›®æ ‡æ•°æ®æ£€æŸ¥å‡ºé”™: {e}")
        else:
            if logger:
                logger.info("3. è·³è¿‡æ°´è´¨ç›®æ ‡æ•°æ®æ£€æŸ¥ (æ•°æ®ä¸å¯ç”¨)")
    else:
        if logger:
            logger.info("è·³è¿‡æ—¶é—´åºåˆ—æ•°æ®æ£€æŸ¥ (æ— æŠ½æ ·æ•°æ®)")
    
    # 4. æ£€æŸ¥å±æ€§æ•°æ®ï¼ˆå®Œæ•´æ•°æ®ï¼Œå¯ä»¥ä¿®å¤ï¼‰
    if attr_features:
        if logger:
            fix_status = "å¯ä¿®å¤" if fix_anomalies else "ä»…æ£€æµ‹"
            logger.info(f"4. æ£€æŸ¥æ²³æ®µå±æ€§æ•°æ®ï¼ˆå®Œæ•´æ•°æ®ï¼Œ{fix_status}ï¼‰...")
        try:
            attr_df, attr_results = attr_checker.check_attr_data(
                attr_df, attr_features, exclude_comids
            )
            attr_results['sample_based'] = False
            quality_report['attr_data'] = attr_results
        except Exception as e:
            if logger:
                logger.warning(f"å±æ€§æ•°æ®æ£€æŸ¥å‡ºé”™: {e}")
    
    if logger:
        logger.info("åŸºäºæŠ½æ ·çš„æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        
        # æ€»ç»“æŠ¥å‘Š
        anomaly_found = any(report.get('has_anomalies', False) for report in quality_report.values())
        if anomaly_found:
            logger.warning("ğŸ“Š æ£€æŸ¥æ‘˜è¦ï¼šå‘ç°æ•°æ®å¼‚å¸¸")
            logger.info("ğŸ“ æŠ½æ ·æ£€æµ‹ç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…è®­ç»ƒå°†ä½¿ç”¨å®Œæ•´æ•°æ®é›†")
        else:
            logger.info("âœ… æ£€æŸ¥æ‘˜è¦ï¼šæœªå‘ç°æ˜æ˜¾å¼‚å¸¸")
    
    return sample_df, attr_df, quality_report